{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting appnope==0.1.4 (from -r requirements.txt (line 1))\n",
            "  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n",
            "Requirement already satisfied: asttokens==2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.4.1)\n",
            "Collecting certifi==2024.8.30 (from -r requirements.txt (line 3))\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting charset-normalizer==3.4.0 (from -r requirements.txt (line 4))\n",
            "  Downloading charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Collecting comm==0.2.2 (from -r requirements.txt (line 5))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting contourpy==1.3.0 (from -r requirements.txt (line 6))\n",
            "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler==0.12.1 (from -r requirements.txt (line 7))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting debugpy==1.8.7 (from -r requirements.txt (line 8))\n",
            "  Downloading debugpy-1.8.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (5.1.1)\n",
            "Collecting executing==2.1.0 (from -r requirements.txt (line 10))\n",
            "  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting filelock==3.16.1 (from -r requirements.txt (line 11))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fonttools==4.54.1 (from -r requirements.txt (line 12))\n",
            "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2024.10.0 (from -r requirements.txt (line 13))\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting huggingface-hub==0.26.1 (from -r requirements.txt (line 14))\n",
            "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting idna==3.10 (from -r requirements.txt (line 15))\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting ipykernel==6.29.5 (from -r requirements.txt (line 16))\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ipython==8.29.0 (from -r requirements.txt (line 17))\n",
            "  Downloading ipython-8.29.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jedi==0.19.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (0.19.1)\n",
            "Collecting Jinja2==3.1.4 (from -r requirements.txt (line 19))\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting jupyter_client==8.6.3 (from -r requirements.txt (line 20))\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter_core==5.7.2 (from -r requirements.txt (line 21))\n",
            "  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting kiwisolver==1.4.7 (from -r requirements.txt (line 22))\n",
            "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting MarkupSafe==3.0.2 (from -r requirements.txt (line 23))\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting matplotlib==3.9.2 (from -r requirements.txt (line 24))\n",
            "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting matplotlib-inline==0.1.7 (from -r requirements.txt (line 25))\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (1.3.0)\n",
            "Collecting nest-asyncio==1.6.0 (from -r requirements.txt (line 27))\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting networkx==3.4.2 (from -r requirements.txt (line 28))\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting numpy==2.1.2 (from -r requirements.txt (line 29))\n",
            "  Downloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==24.1 (from -r requirements.txt (line 30))\n",
            "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting parso==0.8.4 (from -r requirements.txt (line 31))\n",
            "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting pexpect==4.9.0 (from -r requirements.txt (line 32))\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pillow==11.0.0 (from -r requirements.txt (line 33))\n",
            "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting platformdirs==4.3.6 (from -r requirements.txt (line 34))\n",
            "  Downloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting prompt_toolkit==3.0.48 (from -r requirements.txt (line 35))\n",
            "  Downloading prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting psutil==6.1.0 (from -r requirements.txt (line 36))\n",
            "  Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (0.7.0)\n",
            "Collecting pure_eval==0.2.3 (from -r requirements.txt (line 38))\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting Pygments==2.18.0 (from -r requirements.txt (line 39))\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pyparsing==3.2.0 (from -r requirements.txt (line 40))\n",
            "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 41))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting PyYAML==6.0.2 (from -r requirements.txt (line 42))\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting pyzmq==26.2.0 (from -r requirements.txt (line 43))\n",
            "  Downloading pyzmq-26.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting regex==2024.9.11 (from -r requirements.txt (line 44))\n",
            "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.32.3 (from -r requirements.txt (line 45))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting safetensors==0.4.5 (from -r requirements.txt (line 46))\n",
            "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting setuptools==75.2.0 (from -r requirements.txt (line 47))\n",
            "  Downloading setuptools-75.2.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 48)) (1.16.0)\n",
            "Requirement already satisfied: stack-data==0.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 49)) (0.6.3)\n",
            "Collecting sympy==1.13.1 (from -r requirements.txt (line 50))\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tokenizers==0.20.1 (from -r requirements.txt (line 51))\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting torch==2.5.0 (from -r requirements.txt (line 52))\n",
            "  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.20.0 (from -r requirements.txt (line 53))\n",
            "  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting tornado==6.4.1 (from -r requirements.txt (line 54))\n",
            "  Downloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting tqdm==4.66.5 (from -r requirements.txt (line 55))\n",
            "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traitlets==5.14.3 (from -r requirements.txt (line 56))\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting transformers==4.46.0 (from -r requirements.txt (line 57))\n",
            "  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing_extensions==4.12.2 (from -r requirements.txt (line 58))\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting urllib3==2.2.3 (from -r requirements.txt (line 59))\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting wcwidth==0.2.13 (from -r requirements.txt (line 60))\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pandas (from -r requirements.txt (line 61))\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython==8.29.0->-r requirements.txt (line 17)) (1.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.0->-r requirements.txt (line 52))\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 61))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 61))\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\n",
            "Reason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n",
            "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading debugpy-1.8.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-8.29.0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.9/819.9 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
            "Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
            "Downloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzmq-26.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (868 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-75.2.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wcwidth, pytz, pure_eval, urllib3, tzdata, typing_extensions, traitlets, tqdm, tornado, sympy, setuptools, safetensors, regex, pyzmq, PyYAML, python-dateutil, pyparsing, Pygments, psutil, prompt_toolkit, platformdirs, pillow, pexpect, parso, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, nest-asyncio, MarkupSafe, kiwisolver, idna, fsspec, fonttools, filelock, executing, debugpy, cycler, charset-normalizer, certifi, appnope, triton, requests, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib-inline, jupyter_core, Jinja2, contourpy, comm, nvidia-cusolver-cu12, matplotlib, jupyter_client, ipython, huggingface-hub, torch, tokenizers, ipykernel, transformers, torchvision\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.9\n",
            "    Uninstalling wcwidth-0.2.9:\n",
            "      Successfully uninstalled wcwidth-0.2.9\n",
            "  Attempting uninstall: pure_eval\n",
            "    Found existing installation: pure-eval 0.2.2\n",
            "    Uninstalling pure-eval-0.2.2:\n",
            "      Successfully uninstalled pure-eval-0.2.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.13\n",
            "    Uninstalling urllib3-1.26.13:\n",
            "      Successfully uninstalled urllib3-1.26.13\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.13.0\n",
            "    Uninstalling traitlets-5.13.0:\n",
            "      Successfully uninstalled traitlets-5.13.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.3.3\n",
            "    Uninstalling tornado-6.3.3:\n",
            "      Successfully uninstalled tornado-6.3.3\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 68.2.2\n",
            "    Uninstalling setuptools-68.2.2:\n",
            "      Successfully uninstalled setuptools-68.2.2\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.16.1\n",
            "    Uninstalling Pygments-2.16.1:\n",
            "      Successfully uninstalled Pygments-2.16.1\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.6\n",
            "    Uninstalling psutil-5.9.6:\n",
            "      Successfully uninstalled psutil-5.9.6\n",
            "  Attempting uninstall: prompt_toolkit\n",
            "    Found existing installation: prompt-toolkit 3.0.39\n",
            "    Uninstalling prompt-toolkit-3.0.39:\n",
            "      Successfully uninstalled prompt-toolkit-3.0.39\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 3.11.0\n",
            "    Uninstalling platformdirs-3.11.0:\n",
            "      Successfully uninstalled platformdirs-3.11.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.3.0\n",
            "    Uninstalling Pillow-9.3.0:\n",
            "      Successfully uninstalled Pillow-9.3.0\n",
            "  Attempting uninstall: pexpect\n",
            "    Found existing installation: pexpect 4.8.0\n",
            "    Uninstalling pexpect-4.8.0:\n",
            "      Successfully uninstalled pexpect-4.8.0\n",
            "  Attempting uninstall: parso\n",
            "    Found existing installation: parso 0.8.3\n",
            "    Uninstalling parso-0.8.3:\n",
            "      Successfully uninstalled parso-0.8.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.1\n",
            "    Uninstalling numpy-1.24.1:\n",
            "      Successfully uninstalled numpy-1.24.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "  Attempting uninstall: nest-asyncio\n",
            "    Found existing installation: nest-asyncio 1.5.8\n",
            "    Uninstalling nest-asyncio-1.5.8:\n",
            "      Successfully uninstalled nest-asyncio-1.5.8\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.2\n",
            "    Uninstalling MarkupSafe-2.1.2:\n",
            "      Successfully uninstalled MarkupSafe-2.1.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.4.0\n",
            "    Uninstalling fsspec-2023.4.0:\n",
            "      Successfully uninstalled fsspec-2023.4.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.9.0\n",
            "    Uninstalling filelock-3.9.0:\n",
            "      Successfully uninstalled filelock-3.9.0\n",
            "  Attempting uninstall: executing\n",
            "    Found existing installation: executing 2.0.1\n",
            "    Uninstalling executing-2.0.1:\n",
            "      Successfully uninstalled executing-2.0.1\n",
            "  Attempting uninstall: debugpy\n",
            "    Found existing installation: debugpy 1.8.0\n",
            "    Uninstalling debugpy-1.8.0:\n",
            "      Successfully uninstalled debugpy-1.8.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.1.1\n",
            "    Uninstalling charset-normalizer-2.1.1:\n",
            "      Successfully uninstalled charset-normalizer-2.1.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.12.7\n",
            "    Uninstalling certifi-2022.12.7:\n",
            "      Successfully uninstalled certifi-2022.12.7\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: matplotlib-inline\n",
            "    Found existing installation: matplotlib-inline 0.1.6\n",
            "    Uninstalling matplotlib-inline-0.1.6:\n",
            "      Successfully uninstalled matplotlib-inline-0.1.6\n",
            "  Attempting uninstall: jupyter_core\n",
            "    Found existing installation: jupyter_core 5.5.0\n",
            "    Uninstalling jupyter_core-5.5.0:\n",
            "      Successfully uninstalled jupyter_core-5.5.0\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 3.1.2\n",
            "    Uninstalling Jinja2-3.1.2:\n",
            "      Successfully uninstalled Jinja2-3.1.2\n",
            "  Attempting uninstall: comm\n",
            "    Found existing installation: comm 0.2.0\n",
            "    Uninstalling comm-0.2.0:\n",
            "      Successfully uninstalled comm-0.2.0\n",
            "  Attempting uninstall: jupyter_client\n",
            "    Found existing installation: jupyter_client 7.4.9\n",
            "    Uninstalling jupyter_client-7.4.9:\n",
            "      Successfully uninstalled jupyter_client-7.4.9\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 8.17.2\n",
            "    Uninstalling ipython-8.17.2:\n",
            "      Successfully uninstalled ipython-8.17.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 6.26.0\n",
            "    Uninstalling ipykernel-6.26.0:\n",
            "      Successfully uninstalled ipykernel-6.26.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu118\n",
            "    Uninstalling torchvision-0.16.0+cu118:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\n",
            "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 26.2.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Jinja2-3.1.4 MarkupSafe-3.0.2 PyYAML-6.0.2 Pygments-2.18.0 appnope-0.1.4 certifi-2024.8.30 charset-normalizer-3.4.0 comm-0.2.2 contourpy-1.3.0 cycler-0.12.1 debugpy-1.8.7 executing-2.1.0 filelock-3.16.1 fonttools-4.54.1 fsspec-2024.10.0 huggingface-hub-0.26.1 idna-3.10 ipykernel-6.29.5 ipython-8.29.0 jupyter_client-8.6.3 jupyter_core-5.7.2 kiwisolver-1.4.7 matplotlib-3.9.2 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 networkx-3.4.2 numpy-2.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 packaging-24.1 pandas-2.2.3 parso-0.8.4 pexpect-4.9.0 pillow-11.0.0 platformdirs-4.3.6 prompt_toolkit-3.0.48 psutil-6.1.0 pure_eval-0.2.3 pyparsing-3.2.0 python-dateutil-2.9.0.post0 pytz-2024.2 pyzmq-26.2.0 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 setuptools-75.2.0 sympy-1.13.1 tokenizers-0.20.1 torch-2.5.0 torchvision-0.20.0 tornado-6.4.1 tqdm-4.66.5 traitlets-5.14.3 transformers-4.46.0 triton-3.1.0 typing_extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 wcwidth-0.2.13\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 64.3MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torchvision\n",
        "\n",
        "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "\n",
        "# data from augmentation ablation here\n",
        "basic_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n",
        "train_fms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
        "                        tt.RandomHorizontalFlip(), \n",
        "                        tt.ToTensor(), \n",
        "                        tt.Normalize(*stats,inplace=True)])\n",
        "\n",
        "batch_size = 1024\n",
        "\n",
        "train_normal = torchvision.datasets.CIFAR10(root='./data', train=True, transform=basic_tfms, download=True)\n",
        "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_fms)\n",
        "\n",
        "train_dataset = ConcatDataset([train_ds, train_normal])\n",
        "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
        "\n",
        "val_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=basic_tfms)\n",
        "valid_dl = DataLoader(val_dataset, batch_size*2, num_workers=3, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running on cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    # elif torch.backends.mps.is_available():\n",
        "    #     return torch.device(\"mps\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "    \n",
        "def clear_cache():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    elif torch.backends.mps.is_available():\n",
        "        torch.mps.empty_cache()\n",
        "    # else:\n",
        "    #     return torch.device(\"cpu\")\n",
        "    # appers there's nothing to do here\n",
        "        \n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "    \n",
        "device = get_default_device()\n",
        "print(f\"running on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "class ResNet9(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = self.conv_block(in_channels, 64)\n",
        "        self.conv2 = self.conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(self.conv_block(128, 128), self.conv_block(128, 128))\n",
        "        \n",
        "        self.conv3 = self.conv_block(128, 256, pool=True)\n",
        "        self.conv4 = self.conv_block(256, 512, pool=True)\n",
        "        self.res2 = nn.Sequential(self.conv_block(512, 512), self.conv_block(512, 512))\n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
        "                                        nn.Flatten(), \n",
        "                                        nn.Linear(512, num_classes))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  \n",
        "        loss = F.cross_entropy(out, labels) \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    \n",
        "        loss = F.cross_entropy(out, labels)   \n",
        "        acc = accuracy(out, labels)           \n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   \n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      \n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, pool=False):\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "                nn.BatchNorm2d(out_channels), \n",
        "                nn.ReLU(inplace=True)]\n",
        "        if pool: \n",
        "            layers.append(nn.MaxPool2d(2))\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = to_device(ResNet9(3, 10), device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    \n",
        "    # Set up cutom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(train_loader),\n",
        "                                                pct_start=0.3)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        #print(f'Allocated: {torch.cuda.memory_allocated() / 1024 ** 2} MB')\n",
        "        #print(f'Cached: {torch.cuda.memory_reserved() / 1024 ** 2} MB')\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            #print(f'Allocated: {torch.cuda.memory_allocated() / 1024 ** 2} MB')\n",
        "            #print(f'Cached: {torch.cuda.memory_reserved() / 1024 ** 2} MB')\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            if grad_clip: \n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 8\n",
        "max_lr = 0.01\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-4\n",
        "opt_func = torch.optim.Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], last_lr: 0.00392, train_loss: 1.1557, val_loss: 0.9121, val_acc: 0.6802\n",
            "Epoch [1], last_lr: 0.00935, train_loss: 0.7715, val_loss: 1.0019, val_acc: 0.6827\n",
            "Epoch [2], last_lr: 0.00972, train_loss: 0.6176, val_loss: 0.6538, val_acc: 0.7893\n",
            "Epoch [3], last_lr: 0.00812, train_loss: 0.3642, val_loss: 0.6018, val_acc: 0.8011\n",
            "Epoch [4], last_lr: 0.00556, train_loss: 0.2459, val_loss: 0.4182, val_acc: 0.8651\n",
            "Epoch [5], last_lr: 0.00283, train_loss: 0.1520, val_loss: 0.2951, val_acc: 0.9032\n",
            "Epoch [6], last_lr: 0.00077, train_loss: 0.0885, val_loss: 0.2665, val_acc: 0.9125\n",
            "Epoch [7], last_lr: 0.00000, train_loss: 0.0561, val_loss: 0.2474, val_acc: 0.9196\n",
            "Epoch [0], last_lr: 0.00327, train_loss: 1.2054, val_loss: 1.0240, val_acc: 0.6577\n",
            "Epoch [1], last_lr: 0.00848, train_loss: 0.7267, val_loss: 1.2581, val_acc: 0.6532\n",
            "Epoch [2], last_lr: 0.00994, train_loss: 0.6725, val_loss: 1.1960, val_acc: 0.6586\n",
            "Epoch [3], last_lr: 0.00899, train_loss: 0.3791, val_loss: 0.6367, val_acc: 0.7977\n",
            "Epoch [4], last_lr: 0.00706, train_loss: 0.2748, val_loss: 0.6604, val_acc: 0.7917\n",
            "Epoch [5], last_lr: 0.00463, train_loss: 0.1888, val_loss: 0.4016, val_acc: 0.8720\n",
            "Epoch [6], last_lr: 0.00229, train_loss: 0.1190, val_loss: 0.2961, val_acc: 0.9026\n",
            "Epoch [7], last_lr: 0.00061, train_loss: 0.0668, val_loss: 0.2388, val_acc: 0.9230\n",
            "Epoch [8], last_lr: 0.00000, train_loss: 0.0439, val_loss: 0.2377, val_acc: 0.9232\n",
            "Epoch [0], last_lr: 0.00277, train_loss: 1.1720, val_loss: 0.9088, val_acc: 0.6781\n",
            "Epoch [1], last_lr: 0.00759, train_loss: 0.7335, val_loss: 1.1056, val_acc: 0.6862\n",
            "Epoch [2], last_lr: 0.01000, train_loss: 0.6376, val_loss: 1.0013, val_acc: 0.6998\n",
            "Epoch [3], last_lr: 0.00950, train_loss: 0.4085, val_loss: 0.6743, val_acc: 0.7814\n",
            "Epoch [4], last_lr: 0.00812, train_loss: 0.3201, val_loss: 0.4944, val_acc: 0.8457\n",
            "Epoch [5], last_lr: 0.00611, train_loss: 0.2195, val_loss: 0.4798, val_acc: 0.8471\n",
            "Epoch [6], last_lr: 0.00389, train_loss: 0.1416, val_loss: 0.4209, val_acc: 0.8706\n",
            "Epoch [7], last_lr: 0.00188, train_loss: 0.0891, val_loss: 0.3068, val_acc: 0.9043\n",
            "Epoch [8], last_lr: 0.00050, train_loss: 0.0555, val_loss: 0.2614, val_acc: 0.9206\n",
            "Epoch [9], last_lr: 0.00000, train_loss: 0.0370, val_loss: 0.2506, val_acc: 0.9239\n",
            "Epoch [0], last_lr: 0.00239, train_loss: 1.2462, val_loss: 1.1961, val_acc: 0.6051\n",
            "Epoch [1], last_lr: 0.00675, train_loss: 0.7248, val_loss: 0.8693, val_acc: 0.6953\n",
            "Epoch [2], last_lr: 0.00980, train_loss: 0.6377, val_loss: 0.9443, val_acc: 0.7184\n",
            "Epoch [3], last_lr: 0.00980, train_loss: 0.4898, val_loss: 0.5251, val_acc: 0.8235\n",
            "Epoch [4], last_lr: 0.00884, train_loss: 0.3107, val_loss: 0.4593, val_acc: 0.8495\n",
            "Epoch [5], last_lr: 0.00726, train_loss: 0.2294, val_loss: 0.4573, val_acc: 0.8522\n",
            "Epoch [6], last_lr: 0.00531, train_loss: 0.1744, val_loss: 0.4202, val_acc: 0.8710\n",
            "Epoch [7], last_lr: 0.00330, train_loss: 0.1149, val_loss: 0.3316, val_acc: 0.8935\n",
            "Epoch [8], last_lr: 0.00157, train_loss: 0.0701, val_loss: 0.2699, val_acc: 0.9193\n",
            "Epoch [9], last_lr: 0.00041, train_loss: 0.0423, val_loss: 0.2344, val_acc: 0.9286\n",
            "Epoch [10], last_lr: 0.00000, train_loss: 0.0298, val_loss: 0.2331, val_acc: 0.9293\n",
            "Epoch [0], last_lr: 0.00209, train_loss: 1.2281, val_loss: 1.4637, val_acc: 0.5770\n",
            "Epoch [1], last_lr: 0.00601, train_loss: 0.7024, val_loss: 3.6068, val_acc: 0.3648\n",
            "Epoch [2], last_lr: 0.00935, train_loss: 0.6192, val_loss: 1.1750, val_acc: 0.6819\n",
            "Epoch [3], last_lr: 0.00994, train_loss: 0.4549, val_loss: 0.8790, val_acc: 0.7365\n",
            "Epoch [4], last_lr: 0.00933, train_loss: 0.3410, val_loss: 0.6704, val_acc: 0.7950\n",
            "Epoch [5], last_lr: 0.00812, train_loss: 0.2550, val_loss: 0.4346, val_acc: 0.8572\n",
            "Epoch [6], last_lr: 0.00647, train_loss: 0.1857, val_loss: 0.4481, val_acc: 0.8619\n",
            "Epoch [7], last_lr: 0.00463, train_loss: 0.1386, val_loss: 0.3856, val_acc: 0.8781\n",
            "Epoch [8], last_lr: 0.00283, train_loss: 0.0949, val_loss: 0.2963, val_acc: 0.9078\n",
            "Epoch [9], last_lr: 0.00133, train_loss: 0.0573, val_loss: 0.2664, val_acc: 0.9192\n",
            "Epoch [10], last_lr: 0.00035, train_loss: 0.0359, val_loss: 0.2312, val_acc: 0.9258\n",
            "Epoch [11], last_lr: 0.00000, train_loss: 0.0252, val_loss: 0.2303, val_acc: 0.9274\n",
            "Epoch [0], last_lr: 0.00185, train_loss: 1.2498, val_loss: 1.0797, val_acc: 0.6120\n",
            "Epoch [1], last_lr: 0.00537, train_loss: 0.7069, val_loss: 1.0290, val_acc: 0.6678\n",
            "Epoch [2], last_lr: 0.00879, train_loss: 0.5870, val_loss: 0.8553, val_acc: 0.7416\n",
            "Epoch [3], last_lr: 0.01000, train_loss: 0.4954, val_loss: 0.8570, val_acc: 0.7416\n",
            "Epoch [4], last_lr: 0.00964, train_loss: 0.3486, val_loss: 0.4393, val_acc: 0.8544\n",
            "Epoch [5], last_lr: 0.00874, train_loss: 0.2632, val_loss: 0.4029, val_acc: 0.8701\n",
            "Epoch [6], last_lr: 0.00740, train_loss: 0.2149, val_loss: 0.4422, val_acc: 0.8620\n",
            "Epoch [7], last_lr: 0.00577, train_loss: 0.1581, val_loss: 0.4196, val_acc: 0.8681\n",
            "Epoch [8], last_lr: 0.00406, train_loss: 0.1138, val_loss: 0.4154, val_acc: 0.8734\n",
            "Epoch [9], last_lr: 0.00245, train_loss: 0.0771, val_loss: 0.2988, val_acc: 0.9086\n",
            "Epoch [10], last_lr: 0.00115, train_loss: 0.0481, val_loss: 0.2673, val_acc: 0.9191\n",
            "Epoch [11], last_lr: 0.00030, train_loss: 0.0314, val_loss: 0.2392, val_acc: 0.9269\n",
            "Epoch [12], last_lr: 0.00000, train_loss: 0.0226, val_loss: 0.2363, val_acc: 0.9287\n",
            "Epoch [0], last_lr: 0.00166, train_loss: 1.2577, val_loss: 1.0869, val_acc: 0.6318\n",
            "Epoch [1], last_lr: 0.00482, train_loss: 0.6993, val_loss: 0.8250, val_acc: 0.7300\n",
            "Epoch [2], last_lr: 0.00818, train_loss: 0.5870, val_loss: 0.5637, val_acc: 0.8126\n",
            "Epoch [3], last_lr: 0.00995, train_loss: 0.5226, val_loss: 0.6659, val_acc: 0.7846\n",
            "Epoch [4], last_lr: 0.00984, train_loss: 0.3727, val_loss: 0.7409, val_acc: 0.7616\n",
            "Epoch [5], last_lr: 0.00919, train_loss: 0.2749, val_loss: 0.8576, val_acc: 0.7434\n",
            "Epoch [6], last_lr: 0.00812, train_loss: 0.2310, val_loss: 0.4919, val_acc: 0.8451\n",
            "Epoch [7], last_lr: 0.00673, train_loss: 0.1780, val_loss: 0.3995, val_acc: 0.8713\n",
            "Epoch [8], last_lr: 0.00516, train_loss: 0.1339, val_loss: 0.4992, val_acc: 0.8468\n",
            "Epoch [9], last_lr: 0.00358, train_loss: 0.0992, val_loss: 0.3257, val_acc: 0.9019\n",
            "Epoch [10], last_lr: 0.00214, train_loss: 0.0629, val_loss: 0.3043, val_acc: 0.9080\n",
            "Epoch [11], last_lr: 0.00099, train_loss: 0.0395, val_loss: 0.2487, val_acc: 0.9267\n",
            "Epoch [12], last_lr: 0.00025, train_loss: 0.0268, val_loss: 0.2466, val_acc: 0.9270\n",
            "Epoch [13], last_lr: 0.00000, train_loss: 0.0207, val_loss: 0.2449, val_acc: 0.9292\n",
            "Epoch [0], last_lr: 0.00151, train_loss: 1.2062, val_loss: 1.0232, val_acc: 0.6425\n",
            "Epoch [1], last_lr: 0.00435, train_loss: 0.6761, val_loss: 0.9383, val_acc: 0.7140\n",
            "Epoch [2], last_lr: 0.00759, train_loss: 0.5817, val_loss: 0.9755, val_acc: 0.7111\n",
            "Epoch [3], last_lr: 0.00971, train_loss: 0.4876, val_loss: 0.8303, val_acc: 0.7610\n",
            "Epoch [4], last_lr: 0.00994, train_loss: 0.3670, val_loss: 0.7039, val_acc: 0.7716\n",
            "Epoch [5], last_lr: 0.00950, train_loss: 0.2886, val_loss: 1.0118, val_acc: 0.7402\n",
            "Epoch [6], last_lr: 0.00867, train_loss: 0.2472, val_loss: 1.0509, val_acc: 0.7213\n",
            "Epoch [7], last_lr: 0.00750, train_loss: 0.1893, val_loss: 0.4441, val_acc: 0.8628\n",
            "Epoch [8], last_lr: 0.00611, train_loss: 0.1524, val_loss: 0.6742, val_acc: 0.8105\n",
            "Epoch [9], last_lr: 0.00463, train_loss: 0.1161, val_loss: 0.3306, val_acc: 0.8932\n",
            "Epoch [10], last_lr: 0.00317, train_loss: 0.0815, val_loss: 0.3397, val_acc: 0.8968\n",
            "Epoch [11], last_lr: 0.00188, train_loss: 0.0564, val_loss: 0.2960, val_acc: 0.9109\n",
            "Epoch [12], last_lr: 0.00087, train_loss: 0.0347, val_loss: 0.2425, val_acc: 0.9258\n",
            "Epoch [13], last_lr: 0.00022, train_loss: 0.0231, val_loss: 0.2399, val_acc: 0.9277\n",
            "Epoch [14], last_lr: 0.00000, train_loss: 0.0179, val_loss: 0.2398, val_acc: 0.9285\n",
            "Epoch [0], last_lr: 0.00138, train_loss: 1.2367, val_loss: 0.9039, val_acc: 0.6813\n",
            "Epoch [1], last_lr: 0.00394, train_loss: 0.6772, val_loss: 1.3744, val_acc: 0.6133\n",
            "Epoch [2], last_lr: 0.00703, train_loss: 0.5386, val_loss: 0.7855, val_acc: 0.7594\n",
            "Epoch [3], last_lr: 0.00935, train_loss: 0.5255, val_loss: 0.7080, val_acc: 0.7834\n",
            "Epoch [4], last_lr: 0.00999, train_loss: 0.3949, val_loss: 0.6900, val_acc: 0.7832\n",
            "Epoch [5], last_lr: 0.00972, train_loss: 0.2967, val_loss: 0.6530, val_acc: 0.8156\n",
            "Epoch [6], last_lr: 0.00908, train_loss: 0.2538, val_loss: 0.4397, val_acc: 0.8603\n",
            "Epoch [7], last_lr: 0.00812, train_loss: 0.1927, val_loss: 0.5330, val_acc: 0.8421\n",
            "Epoch [8], last_lr: 0.00691, train_loss: 0.1781, val_loss: 0.4882, val_acc: 0.8505\n",
            "Epoch [9], last_lr: 0.00556, train_loss: 0.1268, val_loss: 0.5753, val_acc: 0.8396\n",
            "Epoch [10], last_lr: 0.00416, train_loss: 0.1048, val_loss: 0.3490, val_acc: 0.8910\n",
            "Epoch [11], last_lr: 0.00283, train_loss: 0.0725, val_loss: 0.3006, val_acc: 0.9131\n",
            "Epoch [12], last_lr: 0.00167, train_loss: 0.0477, val_loss: 0.2865, val_acc: 0.9170\n",
            "Epoch [13], last_lr: 0.00077, train_loss: 0.0322, val_loss: 0.2457, val_acc: 0.9284\n",
            "Epoch [14], last_lr: 0.00020, train_loss: 0.0209, val_loss: 0.2378, val_acc: 0.9324\n",
            "Epoch [15], last_lr: 0.00000, train_loss: 0.0158, val_loss: 0.2382, val_acc: 0.9330\n",
            "Epoch [0], last_lr: 0.00127, train_loss: 1.2445, val_loss: 1.0583, val_acc: 0.6308\n",
            "Epoch [1], last_lr: 0.00359, train_loss: 0.6820, val_loss: 0.9478, val_acc: 0.6928\n",
            "Epoch [2], last_lr: 0.00650, train_loss: 0.5451, val_loss: 1.0020, val_acc: 0.7173\n",
            "Epoch [3], last_lr: 0.00894, train_loss: 0.4735, val_loss: 0.7941, val_acc: 0.7547\n",
            "Epoch [4], last_lr: 0.00999, train_loss: 0.4390, val_loss: 0.5509, val_acc: 0.8189\n",
            "Epoch [5], last_lr: 0.00986, train_loss: 0.3374, val_loss: 0.6209, val_acc: 0.7997\n",
            "Epoch [6], last_lr: 0.00938, train_loss: 0.2433, val_loss: 0.8107, val_acc: 0.7704\n",
            "Epoch [7], last_lr: 0.00860, train_loss: 0.2004, val_loss: 0.8635, val_acc: 0.7850\n",
            "Epoch [8], last_lr: 0.00758, train_loss: 0.1804, val_loss: 0.5015, val_acc: 0.8440\n",
            "Epoch [9], last_lr: 0.00637, train_loss: 0.1506, val_loss: 0.4851, val_acc: 0.8633\n",
            "Epoch [10], last_lr: 0.00507, train_loss: 0.1163, val_loss: 0.3857, val_acc: 0.8831\n",
            "Epoch [11], last_lr: 0.00376, train_loss: 0.0910, val_loss: 0.3327, val_acc: 0.8960\n",
            "Epoch [12], last_lr: 0.00254, train_loss: 0.0683, val_loss: 0.3063, val_acc: 0.9074\n",
            "Epoch [13], last_lr: 0.00149, train_loss: 0.0434, val_loss: 0.2609, val_acc: 0.9213\n",
            "Epoch [14], last_lr: 0.00068, train_loss: 0.0285, val_loss: 0.2513, val_acc: 0.9287\n",
            "Epoch [15], last_lr: 0.00017, train_loss: 0.0184, val_loss: 0.2468, val_acc: 0.9305\n",
            "Epoch [16], last_lr: 0.00000, train_loss: 0.0150, val_loss: 0.2463, val_acc: 0.9310\n",
            "Epoch [0], last_lr: 0.00118, train_loss: 1.2277, val_loss: 1.2023, val_acc: 0.6196\n",
            "Epoch [1], last_lr: 0.00328, train_loss: 0.7121, val_loss: 1.6789, val_acc: 0.5838\n",
            "Epoch [2], last_lr: 0.00602, train_loss: 0.5486, val_loss: 0.8935, val_acc: 0.7361\n",
            "Epoch [3], last_lr: 0.00849, train_loss: 0.5013, val_loss: 1.1840, val_acc: 0.6675\n",
            "Epoch [4], last_lr: 0.00987, train_loss: 0.4401, val_loss: 0.6762, val_acc: 0.8019\n",
            "Epoch [5], last_lr: 0.00994, train_loss: 0.3152, val_loss: 1.5530, val_acc: 0.6302\n",
            "Epoch [6], last_lr: 0.00961, train_loss: 0.2809, val_loss: 0.4369, val_acc: 0.8574\n",
            "Epoch [7], last_lr: 0.00899, train_loss: 0.2227, val_loss: 0.5854, val_acc: 0.8325\n",
            "Epoch [8], last_lr: 0.00812, train_loss: 0.1892, val_loss: 0.5256, val_acc: 0.8542\n",
            "Epoch [9], last_lr: 0.00706, train_loss: 0.1583, val_loss: 0.5067, val_acc: 0.8552\n",
            "Epoch [10], last_lr: 0.00587, train_loss: 0.1291, val_loss: 0.4630, val_acc: 0.8604\n",
            "Epoch [11], last_lr: 0.00463, train_loss: 0.1083, val_loss: 0.3224, val_acc: 0.8966\n",
            "Epoch [12], last_lr: 0.00341, train_loss: 0.0768, val_loss: 0.3192, val_acc: 0.9074\n",
            "Epoch [13], last_lr: 0.00229, train_loss: 0.0564, val_loss: 0.3033, val_acc: 0.9137\n",
            "Epoch [14], last_lr: 0.00133, train_loss: 0.0363, val_loss: 0.2582, val_acc: 0.9247\n",
            "Epoch [15], last_lr: 0.00061, train_loss: 0.0239, val_loss: 0.2485, val_acc: 0.9294\n",
            "Epoch [16], last_lr: 0.00015, train_loss: 0.0164, val_loss: 0.2414, val_acc: 0.9316\n",
            "Epoch [17], last_lr: 0.00000, train_loss: 0.0131, val_loss: 0.2398, val_acc: 0.9324\n",
            "Epoch [0], last_lr: 0.00110, train_loss: 1.1808, val_loss: 0.8201, val_acc: 0.7082\n",
            "Epoch [1], last_lr: 0.00302, train_loss: 0.6630, val_loss: 1.0268, val_acc: 0.6897\n",
            "Epoch [2], last_lr: 0.00558, train_loss: 0.5201, val_loss: 0.8778, val_acc: 0.7472\n",
            "Epoch [3], last_lr: 0.00804, train_loss: 0.4879, val_loss: 0.8892, val_acc: 0.7373\n",
            "Epoch [4], last_lr: 0.00965, train_loss: 0.4304, val_loss: 0.6578, val_acc: 0.7929\n",
            "Epoch [5], last_lr: 0.00999, train_loss: 0.3417, val_loss: 0.6178, val_acc: 0.8048\n",
            "Epoch [6], last_lr: 0.00977, train_loss: 0.2797, val_loss: 0.4985, val_acc: 0.8360\n",
            "Epoch [7], last_lr: 0.00928, train_loss: 0.2330, val_loss: 0.5817, val_acc: 0.8277\n",
            "Epoch [8], last_lr: 0.00856, train_loss: 0.1952, val_loss: 0.4443, val_acc: 0.8592\n",
            "Epoch [9], last_lr: 0.00764, train_loss: 0.1693, val_loss: 0.4885, val_acc: 0.8511\n",
            "Epoch [10], last_lr: 0.00657, train_loss: 0.1512, val_loss: 0.3413, val_acc: 0.8894\n",
            "Epoch [11], last_lr: 0.00541, train_loss: 0.1153, val_loss: 0.4849, val_acc: 0.8597\n",
            "Epoch [12], last_lr: 0.00424, train_loss: 0.1008, val_loss: 0.3826, val_acc: 0.8858\n",
            "Epoch [13], last_lr: 0.00310, train_loss: 0.0741, val_loss: 0.3051, val_acc: 0.9086\n",
            "Epoch [14], last_lr: 0.00207, train_loss: 0.0507, val_loss: 0.3007, val_acc: 0.9141\n",
            "Epoch [15], last_lr: 0.00120, train_loss: 0.0324, val_loss: 0.2733, val_acc: 0.9199\n",
            "Epoch [16], last_lr: 0.00055, train_loss: 0.0229, val_loss: 0.2562, val_acc: 0.9301\n",
            "Epoch [17], last_lr: 0.00014, train_loss: 0.0152, val_loss: 0.2451, val_acc: 0.9294\n",
            "Epoch [18], last_lr: 0.00000, train_loss: 0.0124, val_loss: 0.2455, val_acc: 0.9307\n",
            "Epoch [0], last_lr: 0.00103, train_loss: 1.2638, val_loss: 0.9739, val_acc: 0.6460\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.7002, val_loss: 0.7354, val_acc: 0.7466\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5410, val_loss: 0.6676, val_acc: 0.7745\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.4938, val_loss: 0.6057, val_acc: 0.7981\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.4278, val_loss: 1.0086, val_acc: 0.7220\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.3414, val_loss: 0.8827, val_acc: 0.7546\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.2905, val_loss: 0.4437, val_acc: 0.8552\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.2421, val_loss: 0.5220, val_acc: 0.8454\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.2118, val_loss: 0.5165, val_acc: 0.8504\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.1808, val_loss: 0.4229, val_acc: 0.8683\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.1576, val_loss: 0.4355, val_acc: 0.8719\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.1395, val_loss: 0.4292, val_acc: 0.8757\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.1088, val_loss: 0.3633, val_acc: 0.8819\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.0832, val_loss: 0.3130, val_acc: 0.9061\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.0664, val_loss: 0.2936, val_acc: 0.9104\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.0474, val_loss: 0.2838, val_acc: 0.9139\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0309, val_loss: 0.2597, val_acc: 0.9250\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0201, val_loss: 0.2366, val_acc: 0.9346\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0142, val_loss: 0.2328, val_acc: 0.9357\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0121, val_loss: 0.2344, val_acc: 0.9360\n"
          ]
        }
      ],
      "source": [
        "# testing epochs\n",
        "history_map = {}\n",
        "index = 0\n",
        "history = []\n",
        "for i in range(8,21):\n",
        "    history = []\n",
        "    model = to_device(ResNet9(3, 10), device)\n",
        "    history += fit_one_cycle(i, max_lr, model, train_dl, valid_dl, \n",
        "                                 grad_clip=grad_clip, \n",
        "                                 weight_decay=weight_decay, \n",
        "                                 opt_func=opt_func)\n",
        "    history_map[index] = history\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Transformation  Final Validation Loss\n",
            "0                8               0.247389\n",
            "1                9               0.237656\n",
            "2               10               0.250610\n",
            "3               11               0.233113\n",
            "4               12               0.230280\n",
            "5               13               0.236255\n",
            "6               14               0.244910\n",
            "7               15               0.239803\n",
            "8               16               0.238151\n",
            "9               17               0.246307\n",
            "10              18               0.239790\n",
            "11              19               0.245538\n",
            "12              20               0.234441\n",
            "**********\n",
            "    Transformation  Final Validation Accuracy\n",
            "0                8                   0.919586\n",
            "1                9                   0.923237\n",
            "2               10                   0.923882\n",
            "3               11                   0.929284\n",
            "4               12                   0.927352\n",
            "5               13                   0.928744\n",
            "6               14                   0.929245\n",
            "7               15                   0.928465\n",
            "8               16                   0.932982\n",
            "9               17                   0.930951\n",
            "10              18                   0.932442\n",
            "11              19                   0.930697\n",
            "12              20                   0.936016\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_final_val_losses(names):\n",
        "    # Create a list of final validation losses\n",
        "    final_val_losses = [history_map[i][-1]['val_loss'] for i in range(len(history_map))]\n",
        "    final_val_acc = [history_map[i][-1]['val_acc'] for i in range(len(history_map))]\n",
        "\n",
        "    # Create a pandas DataFrame and display it\n",
        "    df = pd.DataFrame({\n",
        "        'Transformation': names,\n",
        "        'Final Validation Loss': final_val_losses\n",
        "    })\n",
        "    print(df)\n",
        "    print(\"*\"*10)\n",
        "    df = pd.DataFrame({\n",
        "        'Transformation': names,\n",
        "        'Final Validation Accuracy': final_val_acc\n",
        "    })\n",
        "    print(df)\n",
        "    \n",
        "compare_final_val_losses(range(8,21))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], last_lr: 0.00010, train_loss: 1.4311, val_loss: 1.0412, val_acc: 0.6289\n",
            "Epoch [1], last_lr: 0.00028, train_loss: 0.8788, val_loss: 0.8245, val_acc: 0.7204\n",
            "Epoch [2], last_lr: 0.00052, train_loss: 0.6469, val_loss: 0.9044, val_acc: 0.6986\n",
            "Epoch [3], last_lr: 0.00076, train_loss: 0.4994, val_loss: 0.7206, val_acc: 0.7534\n",
            "Epoch [4], last_lr: 0.00094, train_loss: 0.3980, val_loss: 0.7287, val_acc: 0.7598\n",
            "Epoch [5], last_lr: 0.00100, train_loss: 0.3091, val_loss: 0.6891, val_acc: 0.7742\n",
            "Epoch [6], last_lr: 0.00099, train_loss: 0.2449, val_loss: 0.5677, val_acc: 0.8226\n",
            "Epoch [7], last_lr: 0.00095, train_loss: 0.1914, val_loss: 0.4617, val_acc: 0.8468\n",
            "Epoch [8], last_lr: 0.00089, train_loss: 0.1525, val_loss: 0.4744, val_acc: 0.8458\n",
            "Epoch [9], last_lr: 0.00081, train_loss: 0.1249, val_loss: 0.3626, val_acc: 0.8787\n",
            "Epoch [10], last_lr: 0.00072, train_loss: 0.1034, val_loss: 0.4931, val_acc: 0.8459\n",
            "Epoch [11], last_lr: 0.00061, train_loss: 0.0851, val_loss: 0.3752, val_acc: 0.8806\n",
            "Epoch [12], last_lr: 0.00050, train_loss: 0.0670, val_loss: 0.3488, val_acc: 0.8915\n",
            "Epoch [13], last_lr: 0.00039, train_loss: 0.0546, val_loss: 0.3292, val_acc: 0.8981\n",
            "Epoch [14], last_lr: 0.00028, train_loss: 0.0412, val_loss: 0.3005, val_acc: 0.9069\n",
            "Epoch [15], last_lr: 0.00019, train_loss: 0.0335, val_loss: 0.2834, val_acc: 0.9108\n",
            "Epoch [16], last_lr: 0.00011, train_loss: 0.0266, val_loss: 0.2721, val_acc: 0.9150\n",
            "Epoch [17], last_lr: 0.00005, train_loss: 0.0212, val_loss: 0.2613, val_acc: 0.9189\n",
            "Epoch [18], last_lr: 0.00001, train_loss: 0.0198, val_loss: 0.2576, val_acc: 0.9202\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0177, val_loss: 0.2571, val_acc: 0.9198\n",
            "Epoch [0], last_lr: 0.00021, train_loss: 1.3450, val_loss: 1.0027, val_acc: 0.6458\n",
            "Epoch [1], last_lr: 0.00056, train_loss: 0.7845, val_loss: 0.7554, val_acc: 0.7425\n",
            "Epoch [2], last_lr: 0.00104, train_loss: 0.5646, val_loss: 1.0437, val_acc: 0.6769\n",
            "Epoch [3], last_lr: 0.00152, train_loss: 0.4484, val_loss: 0.5618, val_acc: 0.8073\n",
            "Epoch [4], last_lr: 0.00187, train_loss: 0.3517, val_loss: 0.5668, val_acc: 0.8149\n",
            "Epoch [5], last_lr: 0.00200, train_loss: 0.2817, val_loss: 0.6468, val_acc: 0.7934\n",
            "Epoch [6], last_lr: 0.00197, train_loss: 0.2164, val_loss: 0.4118, val_acc: 0.8580\n",
            "Epoch [7], last_lr: 0.00190, train_loss: 0.1715, val_loss: 0.4273, val_acc: 0.8612\n",
            "Epoch [8], last_lr: 0.00178, train_loss: 0.1400, val_loss: 0.4206, val_acc: 0.8650\n",
            "Epoch [9], last_lr: 0.00162, train_loss: 0.1164, val_loss: 0.4751, val_acc: 0.8554\n",
            "Epoch [10], last_lr: 0.00143, train_loss: 0.0937, val_loss: 0.3878, val_acc: 0.8768\n",
            "Epoch [11], last_lr: 0.00122, train_loss: 0.0743, val_loss: 0.3569, val_acc: 0.8891\n",
            "Epoch [12], last_lr: 0.00100, train_loss: 0.0594, val_loss: 0.3249, val_acc: 0.8980\n",
            "Epoch [13], last_lr: 0.00078, train_loss: 0.0492, val_loss: 0.3279, val_acc: 0.9004\n",
            "Epoch [14], last_lr: 0.00057, train_loss: 0.0354, val_loss: 0.2748, val_acc: 0.9159\n",
            "Epoch [15], last_lr: 0.00038, train_loss: 0.0267, val_loss: 0.2660, val_acc: 0.9187\n",
            "Epoch [16], last_lr: 0.00022, train_loss: 0.0196, val_loss: 0.2558, val_acc: 0.9215\n",
            "Epoch [17], last_lr: 0.00010, train_loss: 0.0158, val_loss: 0.2438, val_acc: 0.9235\n",
            "Epoch [18], last_lr: 0.00003, train_loss: 0.0133, val_loss: 0.2426, val_acc: 0.9251\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0124, val_loss: 0.2416, val_acc: 0.9251\n",
            "Epoch [0], last_lr: 0.00031, train_loss: 1.2754, val_loss: 1.0519, val_acc: 0.6314\n",
            "Epoch [1], last_lr: 0.00084, train_loss: 0.7476, val_loss: 0.6477, val_acc: 0.7756\n",
            "Epoch [2], last_lr: 0.00156, train_loss: 0.5343, val_loss: 0.7254, val_acc: 0.7531\n",
            "Epoch [3], last_lr: 0.00228, train_loss: 0.4272, val_loss: 0.6825, val_acc: 0.7794\n",
            "Epoch [4], last_lr: 0.00281, train_loss: 0.3448, val_loss: 0.6241, val_acc: 0.7979\n",
            "Epoch [5], last_lr: 0.00300, train_loss: 0.2794, val_loss: 0.6122, val_acc: 0.8015\n",
            "Epoch [6], last_lr: 0.00296, train_loss: 0.2246, val_loss: 0.4524, val_acc: 0.8522\n",
            "Epoch [7], last_lr: 0.00285, train_loss: 0.1811, val_loss: 0.4352, val_acc: 0.8572\n",
            "Epoch [8], last_lr: 0.00267, train_loss: 0.1423, val_loss: 0.4307, val_acc: 0.8576\n",
            "Epoch [9], last_lr: 0.00244, train_loss: 0.1120, val_loss: 0.4123, val_acc: 0.8743\n",
            "Epoch [10], last_lr: 0.00215, train_loss: 0.0956, val_loss: 0.4238, val_acc: 0.8736\n",
            "Epoch [11], last_lr: 0.00183, train_loss: 0.0755, val_loss: 0.3720, val_acc: 0.8868\n",
            "Epoch [12], last_lr: 0.00150, train_loss: 0.0631, val_loss: 0.4795, val_acc: 0.8708\n",
            "Epoch [13], last_lr: 0.00117, train_loss: 0.0488, val_loss: 0.3177, val_acc: 0.9059\n",
            "Epoch [14], last_lr: 0.00085, train_loss: 0.0347, val_loss: 0.2601, val_acc: 0.9219\n",
            "Epoch [15], last_lr: 0.00056, train_loss: 0.0259, val_loss: 0.2813, val_acc: 0.9194\n",
            "Epoch [16], last_lr: 0.00033, train_loss: 0.0185, val_loss: 0.2499, val_acc: 0.9300\n",
            "Epoch [17], last_lr: 0.00015, train_loss: 0.0141, val_loss: 0.2387, val_acc: 0.9321\n",
            "Epoch [18], last_lr: 0.00004, train_loss: 0.0116, val_loss: 0.2352, val_acc: 0.9323\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0111, val_loss: 0.2355, val_acc: 0.9324\n",
            "Epoch [0], last_lr: 0.00041, train_loss: 1.2401, val_loss: 0.9172, val_acc: 0.6784\n",
            "Epoch [1], last_lr: 0.00111, train_loss: 0.7070, val_loss: 0.8159, val_acc: 0.7277\n",
            "Epoch [2], last_lr: 0.00207, train_loss: 0.5220, val_loss: 0.6006, val_acc: 0.8029\n",
            "Epoch [3], last_lr: 0.00304, train_loss: 0.4241, val_loss: 0.8206, val_acc: 0.7392\n",
            "Epoch [4], last_lr: 0.00374, train_loss: 0.3470, val_loss: 0.6076, val_acc: 0.8113\n",
            "Epoch [5], last_lr: 0.00400, train_loss: 0.2808, val_loss: 0.7118, val_acc: 0.7843\n",
            "Epoch [6], last_lr: 0.00395, train_loss: 0.2238, val_loss: 0.6055, val_acc: 0.8202\n",
            "Epoch [7], last_lr: 0.00380, train_loss: 0.1860, val_loss: 0.4021, val_acc: 0.8696\n",
            "Epoch [8], last_lr: 0.00356, train_loss: 0.1422, val_loss: 0.4675, val_acc: 0.8610\n",
            "Epoch [9], last_lr: 0.00325, train_loss: 0.1241, val_loss: 0.5923, val_acc: 0.8235\n",
            "Epoch [10], last_lr: 0.00287, train_loss: 0.1091, val_loss: 0.4188, val_acc: 0.8716\n",
            "Epoch [11], last_lr: 0.00245, train_loss: 0.0822, val_loss: 0.3496, val_acc: 0.8918\n",
            "Epoch [12], last_lr: 0.00200, train_loss: 0.0604, val_loss: 0.3410, val_acc: 0.9026\n",
            "Epoch [13], last_lr: 0.00155, train_loss: 0.0485, val_loss: 0.3617, val_acc: 0.9006\n",
            "Epoch [14], last_lr: 0.00113, train_loss: 0.0373, val_loss: 0.2781, val_acc: 0.9169\n",
            "Epoch [15], last_lr: 0.00075, train_loss: 0.0259, val_loss: 0.2688, val_acc: 0.9219\n",
            "Epoch [16], last_lr: 0.00044, train_loss: 0.0186, val_loss: 0.2463, val_acc: 0.9282\n",
            "Epoch [17], last_lr: 0.00020, train_loss: 0.0134, val_loss: 0.2386, val_acc: 0.9304\n",
            "Epoch [18], last_lr: 0.00005, train_loss: 0.0103, val_loss: 0.2379, val_acc: 0.9321\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0096, val_loss: 0.2378, val_acc: 0.9317\n",
            "Epoch [0], last_lr: 0.00052, train_loss: 1.1600, val_loss: 0.9686, val_acc: 0.6721\n",
            "Epoch [1], last_lr: 0.00139, train_loss: 0.6818, val_loss: 0.6629, val_acc: 0.7687\n",
            "Epoch [2], last_lr: 0.00259, train_loss: 0.5096, val_loss: 1.1077, val_acc: 0.6837\n",
            "Epoch [3], last_lr: 0.00380, train_loss: 0.4144, val_loss: 1.2915, val_acc: 0.6503\n",
            "Epoch [4], last_lr: 0.00468, train_loss: 0.3701, val_loss: 0.6684, val_acc: 0.7981\n",
            "Epoch [5], last_lr: 0.00500, train_loss: 0.2993, val_loss: 0.5337, val_acc: 0.8303\n",
            "Epoch [6], last_lr: 0.00494, train_loss: 0.2326, val_loss: 0.5625, val_acc: 0.8204\n",
            "Epoch [7], last_lr: 0.00475, train_loss: 0.1845, val_loss: 0.4693, val_acc: 0.8561\n",
            "Epoch [8], last_lr: 0.00445, train_loss: 0.1525, val_loss: 0.5793, val_acc: 0.8383\n",
            "Epoch [9], last_lr: 0.00406, train_loss: 0.1275, val_loss: 0.4888, val_acc: 0.8563\n",
            "Epoch [10], last_lr: 0.00358, train_loss: 0.1086, val_loss: 0.6110, val_acc: 0.8366\n",
            "Epoch [11], last_lr: 0.00306, train_loss: 0.0920, val_loss: 0.3319, val_acc: 0.8993\n",
            "Epoch [12], last_lr: 0.00250, train_loss: 0.0712, val_loss: 0.3919, val_acc: 0.8836\n",
            "Epoch [13], last_lr: 0.00194, train_loss: 0.0552, val_loss: 0.3234, val_acc: 0.9041\n",
            "Epoch [14], last_lr: 0.00142, train_loss: 0.0388, val_loss: 0.2897, val_acc: 0.9172\n",
            "Epoch [15], last_lr: 0.00094, train_loss: 0.0291, val_loss: 0.2614, val_acc: 0.9265\n",
            "Epoch [16], last_lr: 0.00055, train_loss: 0.0200, val_loss: 0.2622, val_acc: 0.9269\n",
            "Epoch [17], last_lr: 0.00025, train_loss: 0.0133, val_loss: 0.2493, val_acc: 0.9307\n",
            "Epoch [18], last_lr: 0.00006, train_loss: 0.0106, val_loss: 0.2439, val_acc: 0.9327\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0090, val_loss: 0.2443, val_acc: 0.9335\n",
            "Epoch [0], last_lr: 0.00062, train_loss: 1.2484, val_loss: 0.9545, val_acc: 0.6663\n",
            "Epoch [1], last_lr: 0.00167, train_loss: 0.7028, val_loss: 0.7529, val_acc: 0.7350\n",
            "Epoch [2], last_lr: 0.00311, train_loss: 0.5150, val_loss: 0.7332, val_acc: 0.7646\n",
            "Epoch [3], last_lr: 0.00456, train_loss: 0.4311, val_loss: 0.8267, val_acc: 0.7556\n",
            "Epoch [4], last_lr: 0.00561, train_loss: 0.3621, val_loss: 0.4793, val_acc: 0.8428\n",
            "Epoch [5], last_lr: 0.00600, train_loss: 0.3116, val_loss: 0.5751, val_acc: 0.8191\n",
            "Epoch [6], last_lr: 0.00592, train_loss: 0.2375, val_loss: 0.4231, val_acc: 0.8590\n",
            "Epoch [7], last_lr: 0.00570, train_loss: 0.2093, val_loss: 0.4475, val_acc: 0.8538\n",
            "Epoch [8], last_lr: 0.00535, train_loss: 0.1706, val_loss: 0.4315, val_acc: 0.8647\n",
            "Epoch [9], last_lr: 0.00487, train_loss: 0.1399, val_loss: 0.3810, val_acc: 0.8815\n",
            "Epoch [10], last_lr: 0.00430, train_loss: 0.1145, val_loss: 0.3848, val_acc: 0.8822\n",
            "Epoch [11], last_lr: 0.00367, train_loss: 0.0936, val_loss: 0.3523, val_acc: 0.8913\n",
            "Epoch [12], last_lr: 0.00300, train_loss: 0.0819, val_loss: 0.3494, val_acc: 0.8937\n",
            "Epoch [13], last_lr: 0.00233, train_loss: 0.0618, val_loss: 0.3046, val_acc: 0.9099\n",
            "Epoch [14], last_lr: 0.00170, train_loss: 0.0453, val_loss: 0.2868, val_acc: 0.9109\n",
            "Epoch [15], last_lr: 0.00113, train_loss: 0.0328, val_loss: 0.2640, val_acc: 0.9213\n",
            "Epoch [16], last_lr: 0.00065, train_loss: 0.0209, val_loss: 0.2506, val_acc: 0.9262\n",
            "Epoch [17], last_lr: 0.00030, train_loss: 0.0148, val_loss: 0.2430, val_acc: 0.9296\n",
            "Epoch [18], last_lr: 0.00008, train_loss: 0.0111, val_loss: 0.2385, val_acc: 0.9302\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0103, val_loss: 0.2374, val_acc: 0.9309\n",
            "Epoch [0], last_lr: 0.00072, train_loss: 1.1671, val_loss: 1.0102, val_acc: 0.6474\n",
            "Epoch [1], last_lr: 0.00195, train_loss: 0.6662, val_loss: 0.6845, val_acc: 0.7754\n",
            "Epoch [2], last_lr: 0.00363, train_loss: 0.5068, val_loss: 0.8931, val_acc: 0.7154\n",
            "Epoch [3], last_lr: 0.00531, train_loss: 0.4387, val_loss: 1.1466, val_acc: 0.6884\n",
            "Epoch [4], last_lr: 0.00655, train_loss: 0.3747, val_loss: 0.7231, val_acc: 0.7792\n",
            "Epoch [5], last_lr: 0.00700, train_loss: 0.3414, val_loss: 0.7074, val_acc: 0.7781\n",
            "Epoch [6], last_lr: 0.00691, train_loss: 0.2516, val_loss: 0.5340, val_acc: 0.8456\n",
            "Epoch [7], last_lr: 0.00665, train_loss: 0.2113, val_loss: 0.5687, val_acc: 0.8311\n",
            "Epoch [8], last_lr: 0.00624, train_loss: 0.1716, val_loss: 0.4236, val_acc: 0.8724\n",
            "Epoch [9], last_lr: 0.00568, train_loss: 0.1458, val_loss: 0.3899, val_acc: 0.8763\n",
            "Epoch [10], last_lr: 0.00502, train_loss: 0.1270, val_loss: 0.4546, val_acc: 0.8673\n",
            "Epoch [11], last_lr: 0.00428, train_loss: 0.1097, val_loss: 0.5925, val_acc: 0.8427\n",
            "Epoch [12], last_lr: 0.00350, train_loss: 0.0894, val_loss: 0.3479, val_acc: 0.8984\n",
            "Epoch [13], last_lr: 0.00272, train_loss: 0.0671, val_loss: 0.3403, val_acc: 0.9016\n",
            "Epoch [14], last_lr: 0.00198, train_loss: 0.0515, val_loss: 0.2770, val_acc: 0.9178\n",
            "Epoch [15], last_lr: 0.00132, train_loss: 0.0356, val_loss: 0.2543, val_acc: 0.9240\n",
            "Epoch [16], last_lr: 0.00076, train_loss: 0.0240, val_loss: 0.2669, val_acc: 0.9270\n",
            "Epoch [17], last_lr: 0.00035, train_loss: 0.0153, val_loss: 0.2436, val_acc: 0.9315\n",
            "Epoch [18], last_lr: 0.00009, train_loss: 0.0118, val_loss: 0.2424, val_acc: 0.9321\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0107, val_loss: 0.2425, val_acc: 0.9333\n",
            "Epoch [0], last_lr: 0.00083, train_loss: 1.1958, val_loss: 0.8934, val_acc: 0.6895\n",
            "Epoch [1], last_lr: 0.00223, train_loss: 0.6758, val_loss: 0.8157, val_acc: 0.7259\n",
            "Epoch [2], last_lr: 0.00415, train_loss: 0.5230, val_loss: 0.7719, val_acc: 0.7475\n",
            "Epoch [3], last_lr: 0.00607, train_loss: 0.4471, val_loss: 1.0049, val_acc: 0.7039\n",
            "Epoch [4], last_lr: 0.00748, train_loss: 0.3846, val_loss: 0.8189, val_acc: 0.7375\n",
            "Epoch [5], last_lr: 0.00800, train_loss: 0.3206, val_loss: 0.7989, val_acc: 0.7631\n",
            "Epoch [6], last_lr: 0.00790, train_loss: 0.2992, val_loss: 0.4855, val_acc: 0.8410\n",
            "Epoch [7], last_lr: 0.00760, train_loss: 0.2071, val_loss: 0.3905, val_acc: 0.8691\n",
            "Epoch [8], last_lr: 0.00713, train_loss: 0.1801, val_loss: 0.4113, val_acc: 0.8698\n",
            "Epoch [9], last_lr: 0.00649, train_loss: 0.1575, val_loss: 0.3996, val_acc: 0.8773\n",
            "Epoch [10], last_lr: 0.00574, train_loss: 0.1360, val_loss: 0.5169, val_acc: 0.8520\n",
            "Epoch [11], last_lr: 0.00489, train_loss: 0.1168, val_loss: 0.4293, val_acc: 0.8724\n",
            "Epoch [12], last_lr: 0.00400, train_loss: 0.0907, val_loss: 0.3367, val_acc: 0.8972\n",
            "Epoch [13], last_lr: 0.00311, train_loss: 0.0756, val_loss: 0.2929, val_acc: 0.9095\n",
            "Epoch [14], last_lr: 0.00226, train_loss: 0.0536, val_loss: 0.3215, val_acc: 0.9079\n",
            "Epoch [15], last_lr: 0.00151, train_loss: 0.0413, val_loss: 0.2848, val_acc: 0.9167\n",
            "Epoch [16], last_lr: 0.00087, train_loss: 0.0267, val_loss: 0.2358, val_acc: 0.9326\n",
            "Epoch [17], last_lr: 0.00040, train_loss: 0.0170, val_loss: 0.2308, val_acc: 0.9313\n",
            "Epoch [18], last_lr: 0.00010, train_loss: 0.0123, val_loss: 0.2275, val_acc: 0.9352\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0104, val_loss: 0.2269, val_acc: 0.9338\n",
            "Epoch [0], last_lr: 0.00093, train_loss: 1.2332, val_loss: 0.9620, val_acc: 0.6628\n",
            "Epoch [1], last_lr: 0.00251, train_loss: 0.6884, val_loss: 1.3409, val_acc: 0.6254\n",
            "Epoch [2], last_lr: 0.00467, train_loss: 0.5228, val_loss: 0.6904, val_acc: 0.7728\n",
            "Epoch [3], last_lr: 0.00683, train_loss: 0.4466, val_loss: 0.7009, val_acc: 0.7856\n",
            "Epoch [4], last_lr: 0.00842, train_loss: 0.4344, val_loss: 1.1346, val_acc: 0.7427\n",
            "Epoch [5], last_lr: 0.00900, train_loss: 0.3510, val_loss: 0.6745, val_acc: 0.7737\n",
            "Epoch [6], last_lr: 0.00889, train_loss: 0.2821, val_loss: 0.5539, val_acc: 0.8201\n",
            "Epoch [7], last_lr: 0.00855, train_loss: 0.2122, val_loss: 0.5840, val_acc: 0.8388\n",
            "Epoch [8], last_lr: 0.00802, train_loss: 0.1891, val_loss: 0.5766, val_acc: 0.8322\n",
            "Epoch [9], last_lr: 0.00731, train_loss: 0.1700, val_loss: 0.4841, val_acc: 0.8507\n",
            "Epoch [10], last_lr: 0.00645, train_loss: 0.1347, val_loss: 0.3740, val_acc: 0.8816\n",
            "Epoch [11], last_lr: 0.00550, train_loss: 0.1229, val_loss: 0.2965, val_acc: 0.9056\n",
            "Epoch [12], last_lr: 0.00450, train_loss: 0.0990, val_loss: 0.4139, val_acc: 0.8832\n",
            "Epoch [13], last_lr: 0.00350, train_loss: 0.0825, val_loss: 0.3445, val_acc: 0.8941\n",
            "Epoch [14], last_lr: 0.00255, train_loss: 0.0566, val_loss: 0.2861, val_acc: 0.9123\n",
            "Epoch [15], last_lr: 0.00169, train_loss: 0.0411, val_loss: 0.2845, val_acc: 0.9157\n",
            "Epoch [16], last_lr: 0.00098, train_loss: 0.0277, val_loss: 0.2447, val_acc: 0.9295\n",
            "Epoch [17], last_lr: 0.00045, train_loss: 0.0179, val_loss: 0.2464, val_acc: 0.9332\n",
            "Epoch [18], last_lr: 0.00011, train_loss: 0.0135, val_loss: 0.2398, val_acc: 0.9329\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0110, val_loss: 0.2402, val_acc: 0.9341\n",
            "Epoch [0], last_lr: 0.00103, train_loss: 1.2714, val_loss: 0.9510, val_acc: 0.6686\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.7055, val_loss: 1.2568, val_acc: 0.6501\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5319, val_loss: 0.8288, val_acc: 0.7365\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.4846, val_loss: 0.9745, val_acc: 0.7187\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.4549, val_loss: 0.8590, val_acc: 0.7146\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.3571, val_loss: 0.7364, val_acc: 0.7880\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.2890, val_loss: 0.5430, val_acc: 0.8319\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.2370, val_loss: 0.5787, val_acc: 0.8236\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.2049, val_loss: 0.4508, val_acc: 0.8575\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.1850, val_loss: 0.5504, val_acc: 0.8263\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.1592, val_loss: 0.5463, val_acc: 0.8473\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.1277, val_loss: 0.4076, val_acc: 0.8780\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.1082, val_loss: 0.3434, val_acc: 0.8942\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.0871, val_loss: 0.3366, val_acc: 0.8977\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.0648, val_loss: 0.3360, val_acc: 0.9017\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.0467, val_loss: 0.2800, val_acc: 0.9169\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0311, val_loss: 0.2593, val_acc: 0.9253\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0196, val_loss: 0.2433, val_acc: 0.9305\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0140, val_loss: 0.2365, val_acc: 0.9336\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0120, val_loss: 0.2351, val_acc: 0.9345\n",
            "   Transformation  Final Validation Loss\n",
            "0           0.001               0.257113\n",
            "1           0.002               0.241590\n",
            "2           0.003               0.235496\n",
            "3           0.004               0.237753\n",
            "4           0.005               0.244308\n",
            "5           0.006               0.237394\n",
            "6           0.007               0.242466\n",
            "7           0.008               0.226911\n",
            "8           0.009               0.240192\n",
            "9           0.010               0.235056\n",
            "**********\n",
            "   Transformation  Final Validation Accuracy\n",
            "0           0.001                   0.919807\n",
            "1           0.002                   0.925144\n",
            "2           0.003                   0.932377\n",
            "3           0.004                   0.931725\n",
            "4           0.005                   0.933535\n",
            "5           0.006                   0.930873\n",
            "6           0.007                   0.933308\n",
            "7           0.008                   0.933815\n",
            "8           0.009                   0.934122\n",
            "9           0.010                   0.934487\n"
          ]
        }
      ],
      "source": [
        "# best lr\n",
        "\n",
        "import numpy as np\n",
        "best_epoch = 20\n",
        "history_map = {}\n",
        "index = 0\n",
        "\n",
        "for i in np.arange(0.001, 0.011, 0.001):\n",
        "    history = []\n",
        "    model = to_device(ResNet9(3, 10), device)\n",
        "    history += fit_one_cycle(best_epoch, i, model, train_dl, valid_dl, \n",
        "                                 grad_clip=grad_clip, \n",
        "                                 weight_decay=weight_decay, \n",
        "                                 opt_func=opt_func)\n",
        "    history_map[index] = history\n",
        "    index += 1\n",
        "\n",
        "compare_final_val_losses(np.arange(0.001, 0.011, 0.001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], last_lr: 0.00103, train_loss: 1.2177, val_loss: 0.9754, val_acc: 0.6544\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.6893, val_loss: 0.6192, val_acc: 0.7821\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5214, val_loss: 0.7510, val_acc: 0.7533\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.4429, val_loss: 1.0602, val_acc: 0.7164\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.4036, val_loss: 0.8116, val_acc: 0.7471\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.3566, val_loss: 0.5698, val_acc: 0.8214\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.2759, val_loss: 0.8872, val_acc: 0.7623\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.2423, val_loss: 0.5699, val_acc: 0.8205\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.2010, val_loss: 0.5921, val_acc: 0.8188\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.1703, val_loss: 0.6214, val_acc: 0.8233\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.1625, val_loss: 0.4328, val_acc: 0.8683\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.1308, val_loss: 0.4220, val_acc: 0.8740\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.1023, val_loss: 0.3071, val_acc: 0.9047\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.0841, val_loss: 0.3492, val_acc: 0.8960\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.0642, val_loss: 0.2996, val_acc: 0.9116\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.0431, val_loss: 0.2930, val_acc: 0.9133\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0304, val_loss: 0.2660, val_acc: 0.9237\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0194, val_loss: 0.2539, val_acc: 0.9285\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0146, val_loss: 0.2459, val_acc: 0.9312\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0117, val_loss: 0.2444, val_acc: 0.9316\n",
            "Epoch [0], last_lr: 0.00103, train_loss: 1.2742, val_loss: 0.8652, val_acc: 0.6975\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.7040, val_loss: 0.8771, val_acc: 0.7071\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5398, val_loss: 0.8796, val_acc: 0.7272\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.4906, val_loss: 0.5973, val_acc: 0.8020\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.4399, val_loss: 0.9823, val_acc: 0.6872\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.3772, val_loss: 0.9106, val_acc: 0.7347\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.3340, val_loss: 0.6760, val_acc: 0.7997\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.2746, val_loss: 0.7009, val_acc: 0.7999\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.2579, val_loss: 0.6349, val_acc: 0.8022\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.2316, val_loss: 0.6645, val_acc: 0.8181\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.2194, val_loss: 0.7204, val_acc: 0.7917\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.1840, val_loss: 0.7499, val_acc: 0.8005\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.1531, val_loss: 0.4427, val_acc: 0.8549\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.1289, val_loss: 0.3493, val_acc: 0.8889\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.0930, val_loss: 0.2989, val_acc: 0.9070\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.0696, val_loss: 0.2842, val_acc: 0.9119\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0464, val_loss: 0.2534, val_acc: 0.9224\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0299, val_loss: 0.2598, val_acc: 0.9259\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0194, val_loss: 0.2457, val_acc: 0.9317\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0163, val_loss: 0.2426, val_acc: 0.9326\n",
            "Epoch [0], last_lr: 0.00103, train_loss: 1.3081, val_loss: 0.9360, val_acc: 0.6768\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.7124, val_loss: 0.7248, val_acc: 0.7545\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5440, val_loss: 0.7800, val_acc: 0.7438\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.4893, val_loss: 0.7548, val_acc: 0.7497\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.4998, val_loss: 0.6655, val_acc: 0.7695\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.3971, val_loss: 0.5916, val_acc: 0.7967\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.3528, val_loss: 0.8317, val_acc: 0.7369\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.3316, val_loss: 0.6052, val_acc: 0.8031\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.2842, val_loss: 0.4872, val_acc: 0.8355\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.2777, val_loss: 0.6282, val_acc: 0.8071\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.2376, val_loss: 0.5209, val_acc: 0.8274\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.2120, val_loss: 0.6731, val_acc: 0.7964\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.1867, val_loss: 0.3587, val_acc: 0.8839\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.1594, val_loss: 0.3387, val_acc: 0.8851\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.1188, val_loss: 0.3696, val_acc: 0.8818\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.0927, val_loss: 0.2799, val_acc: 0.9078\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0581, val_loss: 0.2738, val_acc: 0.9160\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0372, val_loss: 0.2396, val_acc: 0.9257\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0252, val_loss: 0.2351, val_acc: 0.9267\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0203, val_loss: 0.2340, val_acc: 0.9279\n",
            "Epoch [0], last_lr: 0.00103, train_loss: 1.2236, val_loss: 1.0337, val_acc: 0.6369\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.6908, val_loss: 0.8844, val_acc: 0.7234\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5423, val_loss: 0.8744, val_acc: 0.7278\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.4856, val_loss: 0.8800, val_acc: 0.7237\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.4772, val_loss: 0.7868, val_acc: 0.7517\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.4341, val_loss: 0.7484, val_acc: 0.7436\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.3685, val_loss: 0.8022, val_acc: 0.7335\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.3424, val_loss: 0.5551, val_acc: 0.8175\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.3152, val_loss: 0.9802, val_acc: 0.6940\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.2938, val_loss: 0.7095, val_acc: 0.7839\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.2789, val_loss: 0.6731, val_acc: 0.7860\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.2456, val_loss: 0.5135, val_acc: 0.8327\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.2211, val_loss: 0.4395, val_acc: 0.8551\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.1911, val_loss: 0.4505, val_acc: 0.8555\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.1480, val_loss: 0.3348, val_acc: 0.8880\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.1079, val_loss: 0.3263, val_acc: 0.8971\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0745, val_loss: 0.2822, val_acc: 0.9159\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0463, val_loss: 0.2471, val_acc: 0.9238\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0312, val_loss: 0.2321, val_acc: 0.9297\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0245, val_loss: 0.2321, val_acc: 0.9326\n",
            "Epoch [0], last_lr: 0.00103, train_loss: 1.3007, val_loss: 0.9282, val_acc: 0.6820\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.7179, val_loss: 0.7807, val_acc: 0.7366\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5508, val_loss: 1.0941, val_acc: 0.6874\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.5149, val_loss: 0.9215, val_acc: 0.6987\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.4737, val_loss: 0.6568, val_acc: 0.7810\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.4420, val_loss: 0.8795, val_acc: 0.7100\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.4006, val_loss: 0.7811, val_acc: 0.7400\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.3569, val_loss: 0.6475, val_acc: 0.7901\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.3397, val_loss: 0.9153, val_acc: 0.7319\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.3203, val_loss: 0.7607, val_acc: 0.7619\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.3006, val_loss: 0.7254, val_acc: 0.7700\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.2715, val_loss: 0.4957, val_acc: 0.8287\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.2457, val_loss: 0.4677, val_acc: 0.8482\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.2062, val_loss: 0.5131, val_acc: 0.8332\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.1675, val_loss: 0.3620, val_acc: 0.8785\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.1306, val_loss: 0.2969, val_acc: 0.9032\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0868, val_loss: 0.2729, val_acc: 0.9118\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0555, val_loss: 0.2419, val_acc: 0.9236\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0358, val_loss: 0.2211, val_acc: 0.9305\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0283, val_loss: 0.2203, val_acc: 0.9321\n"
          ]
        }
      ],
      "source": [
        "# best weight decay\n",
        "\n",
        "import numpy as np\n",
        "best_lr = 0.01\n",
        "history_map = {}\n",
        "index = 0\n",
        "\n",
        "for i in np.arange(1e-4, 6e-4, 1e-4):\n",
        "    history = []\n",
        "    model = to_device(ResNet9(3, 10), device)\n",
        "    history += fit_one_cycle(best_epoch, best_lr, model, train_dl, valid_dl, \n",
        "                                 grad_clip=grad_clip, \n",
        "                                 weight_decay=i, \n",
        "                                 opt_func=opt_func)\n",
        "    history_map[index] = history\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Transformation  Final Validation Loss\n",
            "0          0.0001               0.244382\n",
            "1          0.0002               0.242561\n",
            "2          0.0003               0.233962\n",
            "3          0.0004               0.232100\n",
            "4          0.0005               0.220281\n",
            "**********\n",
            "   Transformation  Final Validation Accuracy\n",
            "0          0.0001                   0.931589\n",
            "1          0.0002                   0.932585\n",
            "2          0.0003                   0.927938\n",
            "3          0.0004                   0.932571\n",
            "4          0.0005                   0.932130\n"
          ]
        }
      ],
      "source": [
        "compare_final_val_losses(np.arange(1e-4, 6e-4, 1e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], last_lr: 0.00103, train_loss: 1.2340, val_loss: 1.1875, val_acc: 0.5924\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.6994, val_loss: 0.8359, val_acc: 0.7318\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5430, val_loss: 0.8894, val_acc: 0.7189\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.5108, val_loss: 1.0892, val_acc: 0.6441\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.4916, val_loss: 0.8881, val_acc: 0.6972\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.4443, val_loss: 1.0657, val_acc: 0.6696\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.3957, val_loss: 0.6484, val_acc: 0.7813\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.3591, val_loss: 0.7255, val_acc: 0.7660\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.3516, val_loss: 0.8823, val_acc: 0.7378\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.3243, val_loss: 0.5837, val_acc: 0.8051\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.3022, val_loss: 0.7355, val_acc: 0.7524\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.2748, val_loss: 0.5755, val_acc: 0.8122\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.2428, val_loss: 0.4573, val_acc: 0.8488\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.2116, val_loss: 0.4112, val_acc: 0.8641\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.1645, val_loss: 0.3868, val_acc: 0.8760\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.1292, val_loss: 0.2956, val_acc: 0.9049\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0870, val_loss: 0.2943, val_acc: 0.9055\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0563, val_loss: 0.2663, val_acc: 0.9198\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0365, val_loss: 0.2370, val_acc: 0.9293\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0278, val_loss: 0.2365, val_acc: 0.9287\n",
            "Epoch [0], last_lr: 0.00103, train_loss: 1.2013, val_loss: 0.8417, val_acc: 0.7005\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 0.6811, val_loss: 0.7688, val_acc: 0.7476\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.5207, val_loss: 0.6462, val_acc: 0.7856\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.4628, val_loss: 1.1038, val_acc: 0.7190\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.3942, val_loss: 0.6213, val_acc: 0.8043\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.3054, val_loss: 0.6960, val_acc: 0.8137\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.2066, val_loss: 0.5355, val_acc: 0.8399\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.1582, val_loss: 0.4933, val_acc: 0.8646\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.1230, val_loss: 0.3444, val_acc: 0.8923\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.0877, val_loss: 0.3280, val_acc: 0.9061\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.0676, val_loss: 0.3409, val_acc: 0.9000\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.0537, val_loss: 0.3352, val_acc: 0.9060\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.0410, val_loss: 0.3172, val_acc: 0.9141\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.0298, val_loss: 0.2928, val_acc: 0.9206\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.0224, val_loss: 0.2790, val_acc: 0.9251\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.0155, val_loss: 0.2851, val_acc: 0.9255\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0128, val_loss: 0.2701, val_acc: 0.9302\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0099, val_loss: 0.2672, val_acc: 0.9299\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0088, val_loss: 0.2677, val_acc: 0.9310\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0081, val_loss: 0.2667, val_acc: 0.9313\n",
            "Epoch [0], last_lr: 0.00103, train_loss: 1.7244, val_loss: 1.2258, val_acc: 0.5600\n",
            "Epoch [1], last_lr: 0.00279, train_loss: 1.0716, val_loss: 1.1311, val_acc: 0.6212\n",
            "Epoch [2], last_lr: 0.00519, train_loss: 0.8273, val_loss: 0.9558, val_acc: 0.6728\n",
            "Epoch [3], last_lr: 0.00759, train_loss: 0.6579, val_loss: 0.7400, val_acc: 0.7541\n",
            "Epoch [4], last_lr: 0.00935, train_loss: 0.5459, val_loss: 0.6384, val_acc: 0.7841\n",
            "Epoch [5], last_lr: 0.01000, train_loss: 0.4485, val_loss: 0.7007, val_acc: 0.7756\n",
            "Epoch [6], last_lr: 0.00987, train_loss: 0.3622, val_loss: 0.6782, val_acc: 0.7885\n",
            "Epoch [7], last_lr: 0.00950, train_loss: 0.3038, val_loss: 0.6141, val_acc: 0.7968\n",
            "Epoch [8], last_lr: 0.00891, train_loss: 0.2566, val_loss: 0.4589, val_acc: 0.8458\n",
            "Epoch [9], last_lr: 0.00812, train_loss: 0.2098, val_loss: 0.5262, val_acc: 0.8295\n",
            "Epoch [10], last_lr: 0.00717, train_loss: 0.1826, val_loss: 0.5742, val_acc: 0.8196\n",
            "Epoch [11], last_lr: 0.00611, train_loss: 0.1621, val_loss: 0.4969, val_acc: 0.8432\n",
            "Epoch [12], last_lr: 0.00500, train_loss: 0.1417, val_loss: 0.4724, val_acc: 0.8561\n",
            "Epoch [13], last_lr: 0.00389, train_loss: 0.1247, val_loss: 0.4188, val_acc: 0.8658\n",
            "Epoch [14], last_lr: 0.00283, train_loss: 0.1117, val_loss: 0.4196, val_acc: 0.8681\n",
            "Epoch [15], last_lr: 0.00188, train_loss: 0.0990, val_loss: 0.4080, val_acc: 0.8697\n",
            "Epoch [16], last_lr: 0.00109, train_loss: 0.0858, val_loss: 0.3904, val_acc: 0.8772\n",
            "Epoch [17], last_lr: 0.00050, train_loss: 0.0761, val_loss: 0.3730, val_acc: 0.8815\n",
            "Epoch [18], last_lr: 0.00013, train_loss: 0.0724, val_loss: 0.3677, val_acc: 0.8833\n",
            "Epoch [19], last_lr: 0.00000, train_loss: 0.0686, val_loss: 0.3656, val_acc: 0.8834\n",
            "  Transformation  Final Validation Loss\n",
            "0           Adam               0.236458\n",
            "1          AdamW               0.266735\n",
            "2            SGD               0.365602\n",
            "**********\n",
            "  Transformation  Final Validation Accuracy\n",
            "0           Adam                   0.928680\n",
            "1          AdamW                   0.931329\n",
            "2            SGD                   0.883443\n"
          ]
        }
      ],
      "source": [
        "# best optimizer\n",
        "\n",
        "import numpy as np\n",
        "best_weight_decay = 5e-4\n",
        "history_map = {}\n",
        "\n",
        "opt_func_list = [\"Adam\", \"AdamW\", \"SGD\"]\n",
        "\n",
        "history = []\n",
        "model = to_device(ResNet9(3, 10), device)\n",
        "history += fit_one_cycle(best_epoch, best_lr, model, train_dl, valid_dl, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=best_weight_decay, \n",
        "                             opt_func=torch.optim.Adam)\n",
        "history_map[0] = history\n",
        "\n",
        "history = []\n",
        "model = to_device(ResNet9(3, 10), device)\n",
        "history += fit_one_cycle(best_epoch, best_lr, model, train_dl, valid_dl, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=best_weight_decay, \n",
        "                             opt_func=torch.optim.AdamW)\n",
        "history_map[1] = history\n",
        "\n",
        "history = []\n",
        "model = to_device(ResNet9(3, 10), device)\n",
        "history += fit_one_cycle(best_epoch, best_lr, model, train_dl, valid_dl, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=best_weight_decay, \n",
        "                             opt_func=torch.optim.SGD)\n",
        "history_map[2] = history\n",
        "\n",
        "compare_final_val_losses(opt_func_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

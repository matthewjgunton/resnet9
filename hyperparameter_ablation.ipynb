{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: appnope==0.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: certifi==2024.8.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer==3.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: comm==0.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: debugpy==1.8.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.8.7)\n",
      "Requirement already satisfied: decorator==5.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (5.1.1)\n",
      "Requirement already satisfied: executing==2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: filelock==3.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.16.1)\n",
      "Requirement already satisfied: fonttools==4.54.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.54.1)\n",
      "Requirement already satisfied: fsspec==2024.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub==0.26.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.26.1)\n",
      "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.29.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (8.29.0)\n",
      "Requirement already satisfied: jedi==0.19.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (0.19.1)\n",
      "Requirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.1.4)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (5.7.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (1.4.7)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (3.9.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (0.1.7)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (3.4.2)\n",
      "Requirement already satisfied: numpy==2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (2.1.2)\n",
      "Requirement already satisfied: packaging==24.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 30)) (24.1)\n",
      "Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (4.9.0)\n",
      "Requirement already satisfied: pillow==11.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (11.0.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (3.0.48)\n",
      "Requirement already satisfied: psutil==6.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (6.1.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (0.2.3)\n",
      "Requirement already satisfied: Pygments==2.18.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 39)) (2.18.0)\n",
      "Requirement already satisfied: pyparsing==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 40)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 41)) (2.9.0.post0)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 43)) (26.2.0)\n",
      "Requirement already satisfied: regex==2024.9.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 44)) (2024.9.11)\n",
      "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 45)) (2.32.3)\n",
      "Requirement already satisfied: safetensors==0.4.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (0.4.5)\n",
      "Requirement already satisfied: setuptools==75.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 47)) (75.2.0)\n",
      "Requirement already satisfied: six==1.16.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 48)) (1.16.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 49)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 50)) (1.13.1)\n",
      "Requirement already satisfied: tokenizers==0.20.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 51)) (0.20.1)\n",
      "Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 52)) (2.5.0)\n",
      "Requirement already satisfied: torchvision==0.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 53)) (0.20.0)\n",
      "Requirement already satisfied: tornado==6.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 54)) (6.4.1)\n",
      "Requirement already satisfied: tqdm==4.66.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 55)) (4.66.5)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 56)) (5.14.3)\n",
      "Requirement already satisfied: transformers==4.46.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 57)) (4.46.0)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 58)) (4.12.2)\n",
      "Requirement already satisfied: urllib3==2.2.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 59)) (2.2.3)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 60)) (0.2.13)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 61)) (2.2.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython==8.29.0->-r requirements.txt (line 17)) (1.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->-r requirements.txt (line 52)) (3.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 61)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 61)) (2024.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "\n",
    "stats = ((0.2860,), (0.3530,))\n",
    "\n",
    "# data from augmentation ablation here\n",
    "basic_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n",
    "train_fms = tt.Compose([tt.RandomCrop(28, padding=4, padding_mode='reflect'), \n",
    "                         tt.RandomHorizontalFlip(p=0.5), \n",
    "                         tt.RandomVerticalFlip(p=0.5),\n",
    "                        tt.ToTensor(), \n",
    "                        tt.Normalize(*stats,inplace=True)])\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "train_normal = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=basic_tfms, download=True)\n",
    "train_ds = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=train_fms)\n",
    "\n",
    "train_dataset = ConcatDataset([train_ds, train_normal])\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=basic_tfms)\n",
    "valid_dl = DataLoader(val_dataset, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # elif torch.backends.mps.is_available():\n",
    "    #     return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "def clear_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    # else:\n",
    "    #     return torch.device(\"cpu\")\n",
    "    # appers there's nothing to do here\n",
    "        \n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "device = get_default_device()\n",
    "print(f\"running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = self.conv_block(in_channels, 64)\n",
    "        self.conv2 = self.conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(self.conv_block(128, 128), self.conv_block(128, 128))\n",
    "        \n",
    "        self.conv3 = self.conv_block(128, 256, pool=True)\n",
    "        self.conv4 = self.conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(self.conv_block(512, 512), self.conv_block(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(3), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(512, num_classes))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  \n",
    "        loss = F.cross_entropy(out, labels) \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels)   \n",
    "        acc = accuracy(out, labels)           \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, pool=False):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "                nn.BatchNorm2d(out_channels), \n",
    "                nn.ReLU(inplace=True)]\n",
    "        if pool: \n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(ResNet9(1, 10), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up cutom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                pct_start=0.3)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        #print(f'Allocated: {torch.cuda.memory_allocated() / 1024 ** 2} MB')\n",
    "        #print(f'Cached: {torch.cuda.memory_reserved() / 1024 ** 2} MB')\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            #print(f'Allocated: {torch.cuda.memory_allocated() / 1024 ** 2} MB')\n",
    "            #print(f'Cached: {torch.cuda.memory_reserved() / 1024 ** 2} MB')\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00393, train_loss: 0.5604, val_loss: 0.5107, val_acc: 0.8248\n",
      "Epoch [1], last_lr: 0.00935, train_loss: 0.4159, val_loss: 0.3119, val_acc: 0.8913\n",
      "Epoch [2], last_lr: 0.00972, train_loss: 0.3124, val_loss: 0.4043, val_acc: 0.8673\n",
      "Epoch [3], last_lr: 0.00812, train_loss: 0.2552, val_loss: 0.2743, val_acc: 0.9068\n",
      "Epoch [4], last_lr: 0.00556, train_loss: 0.2127, val_loss: 0.3023, val_acc: 0.8977\n",
      "Epoch [5], last_lr: 0.00283, train_loss: 0.1742, val_loss: 0.1842, val_acc: 0.9336\n",
      "Epoch [6], last_lr: 0.00077, train_loss: 0.1346, val_loss: 0.1597, val_acc: 0.9429\n",
      "Epoch [7], last_lr: 0.00000, train_loss: 0.1077, val_loss: 0.1530, val_acc: 0.9462\n",
      "Epoch [0], last_lr: 0.00327, train_loss: 0.5480, val_loss: 0.3781, val_acc: 0.8650\n",
      "Epoch [1], last_lr: 0.00848, train_loss: 0.3940, val_loss: 0.4423, val_acc: 0.8571\n",
      "Epoch [2], last_lr: 0.00994, train_loss: 0.3467, val_loss: 0.3338, val_acc: 0.8804\n",
      "Epoch [3], last_lr: 0.00899, train_loss: 0.2629, val_loss: 0.2141, val_acc: 0.9204\n",
      "Epoch [4], last_lr: 0.00706, train_loss: 0.2217, val_loss: 0.3070, val_acc: 0.8856\n",
      "Epoch [5], last_lr: 0.00463, train_loss: 0.1911, val_loss: 0.1920, val_acc: 0.9311\n",
      "Epoch [6], last_lr: 0.00229, train_loss: 0.1579, val_loss: 0.1698, val_acc: 0.9386\n",
      "Epoch [7], last_lr: 0.00061, train_loss: 0.1218, val_loss: 0.1557, val_acc: 0.9461\n",
      "Epoch [8], last_lr: 0.00000, train_loss: 0.0974, val_loss: 0.1550, val_acc: 0.9481\n",
      "Epoch [0], last_lr: 0.00278, train_loss: 0.5700, val_loss: 0.4534, val_acc: 0.8414\n",
      "Epoch [1], last_lr: 0.00759, train_loss: 0.3880, val_loss: 0.4428, val_acc: 0.8533\n",
      "Epoch [2], last_lr: 0.01000, train_loss: 0.3421, val_loss: 0.2808, val_acc: 0.8963\n",
      "Epoch [3], last_lr: 0.00950, train_loss: 0.2651, val_loss: 0.3107, val_acc: 0.8861\n",
      "Epoch [4], last_lr: 0.00812, train_loss: 0.2259, val_loss: 0.2529, val_acc: 0.9128\n",
      "Epoch [5], last_lr: 0.00611, train_loss: 0.2015, val_loss: 0.2026, val_acc: 0.9279\n",
      "Epoch [6], last_lr: 0.00389, train_loss: 0.1740, val_loss: 0.1905, val_acc: 0.9343\n",
      "Epoch [7], last_lr: 0.00188, train_loss: 0.1421, val_loss: 0.1650, val_acc: 0.9407\n",
      "Epoch [8], last_lr: 0.00050, train_loss: 0.1090, val_loss: 0.1526, val_acc: 0.9473\n",
      "Epoch [9], last_lr: 0.00000, train_loss: 0.0905, val_loss: 0.1528, val_acc: 0.9491\n",
      "Epoch [0], last_lr: 0.00239, train_loss: 0.5513, val_loss: 0.3994, val_acc: 0.8592\n",
      "Epoch [1], last_lr: 0.00676, train_loss: 0.3698, val_loss: 0.8954, val_acc: 0.7350\n",
      "Epoch [2], last_lr: 0.00980, train_loss: 0.3553, val_loss: 0.4323, val_acc: 0.8627\n",
      "Epoch [3], last_lr: 0.00980, train_loss: 0.2670, val_loss: 0.4038, val_acc: 0.8567\n",
      "Epoch [4], last_lr: 0.00884, train_loss: 0.2369, val_loss: 0.5078, val_acc: 0.8309\n",
      "Epoch [5], last_lr: 0.00726, train_loss: 0.2131, val_loss: 0.4851, val_acc: 0.8589\n",
      "Epoch [6], last_lr: 0.00531, train_loss: 0.1873, val_loss: 0.1968, val_acc: 0.9267\n",
      "Epoch [7], last_lr: 0.00330, train_loss: 0.1561, val_loss: 0.1740, val_acc: 0.9347\n",
      "Epoch [8], last_lr: 0.00157, train_loss: 0.1279, val_loss: 0.1575, val_acc: 0.9456\n",
      "Epoch [9], last_lr: 0.00041, train_loss: 0.0990, val_loss: 0.1573, val_acc: 0.9470\n",
      "Epoch [10], last_lr: 0.00000, train_loss: 0.0835, val_loss: 0.1555, val_acc: 0.9468\n",
      "Epoch [0], last_lr: 0.00209, train_loss: 0.5454, val_loss: 0.6499, val_acc: 0.7665\n",
      "Epoch [1], last_lr: 0.00602, train_loss: 0.3532, val_loss: 0.5038, val_acc: 0.8415\n",
      "Epoch [2], last_lr: 0.00935, train_loss: 0.3340, val_loss: 0.6485, val_acc: 0.8020\n",
      "Epoch [3], last_lr: 0.00994, train_loss: 0.3188, val_loss: 0.3863, val_acc: 0.8668\n",
      "Epoch [4], last_lr: 0.00933, train_loss: 0.2421, val_loss: 0.2457, val_acc: 0.9129\n",
      "Epoch [5], last_lr: 0.00812, train_loss: 0.2141, val_loss: 0.2427, val_acc: 0.9125\n",
      "Epoch [6], last_lr: 0.00647, train_loss: 0.1965, val_loss: 0.2063, val_acc: 0.9275\n",
      "Epoch [7], last_lr: 0.00463, train_loss: 0.1711, val_loss: 0.1838, val_acc: 0.9339\n",
      "Epoch [8], last_lr: 0.00283, train_loss: 0.1437, val_loss: 0.2290, val_acc: 0.9209\n",
      "Epoch [9], last_lr: 0.00133, train_loss: 0.1182, val_loss: 0.1595, val_acc: 0.9452\n",
      "Epoch [10], last_lr: 0.00035, train_loss: 0.0922, val_loss: 0.1593, val_acc: 0.9473\n",
      "Epoch [11], last_lr: 0.00000, train_loss: 0.0788, val_loss: 0.1570, val_acc: 0.9484\n",
      "Epoch [0], last_lr: 0.00186, train_loss: 0.5285, val_loss: 0.3661, val_acc: 0.8717\n",
      "Epoch [1], last_lr: 0.00538, train_loss: 0.3592, val_loss: 0.3653, val_acc: 0.8780\n",
      "Epoch [2], last_lr: 0.00879, train_loss: 0.3394, val_loss: 0.2640, val_acc: 0.9001\n",
      "Epoch [3], last_lr: 0.01000, train_loss: 0.2948, val_loss: 0.2968, val_acc: 0.8924\n",
      "Epoch [4], last_lr: 0.00964, train_loss: 0.2468, val_loss: 0.3328, val_acc: 0.8873\n",
      "Epoch [5], last_lr: 0.00874, train_loss: 0.2232, val_loss: 0.2375, val_acc: 0.9143\n",
      "Epoch [6], last_lr: 0.00740, train_loss: 0.1994, val_loss: 0.4187, val_acc: 0.8704\n",
      "Epoch [7], last_lr: 0.00577, train_loss: 0.1833, val_loss: 0.2074, val_acc: 0.9259\n",
      "Epoch [8], last_lr: 0.00406, train_loss: 0.1616, val_loss: 0.2500, val_acc: 0.9048\n",
      "Epoch [9], last_lr: 0.00245, train_loss: 0.1349, val_loss: 0.1710, val_acc: 0.9400\n",
      "Epoch [10], last_lr: 0.00115, train_loss: 0.1085, val_loss: 0.1608, val_acc: 0.9438\n",
      "Epoch [11], last_lr: 0.00030, train_loss: 0.0865, val_loss: 0.1563, val_acc: 0.9458\n",
      "Epoch [12], last_lr: 0.00000, train_loss: 0.0742, val_loss: 0.1558, val_acc: 0.9473\n",
      "Epoch [0], last_lr: 0.00167, train_loss: 0.5667, val_loss: 0.3780, val_acc: 0.8562\n",
      "Epoch [1], last_lr: 0.00483, train_loss: 0.3504, val_loss: 0.3359, val_acc: 0.8820\n",
      "Epoch [2], last_lr: 0.00819, train_loss: 0.3203, val_loss: 0.9642, val_acc: 0.7344\n",
      "Epoch [3], last_lr: 0.00995, train_loss: 0.3164, val_loss: 0.3919, val_acc: 0.8658\n",
      "Epoch [4], last_lr: 0.00984, train_loss: 0.2438, val_loss: 0.2819, val_acc: 0.9040\n",
      "Epoch [5], last_lr: 0.00919, train_loss: 0.2281, val_loss: 0.2545, val_acc: 0.9178\n",
      "Epoch [6], last_lr: 0.00812, train_loss: 0.2064, val_loss: 0.2345, val_acc: 0.9183\n",
      "Epoch [7], last_lr: 0.00673, train_loss: 0.1875, val_loss: 0.2168, val_acc: 0.9260\n",
      "Epoch [8], last_lr: 0.00516, train_loss: 0.1725, val_loss: 0.2030, val_acc: 0.9259\n",
      "Epoch [9], last_lr: 0.00358, train_loss: 0.1457, val_loss: 0.1924, val_acc: 0.9322\n",
      "Epoch [10], last_lr: 0.00214, train_loss: 0.1248, val_loss: 0.1764, val_acc: 0.9385\n",
      "Epoch [11], last_lr: 0.00099, train_loss: 0.0999, val_loss: 0.1668, val_acc: 0.9427\n",
      "Epoch [12], last_lr: 0.00025, train_loss: 0.0811, val_loss: 0.1601, val_acc: 0.9491\n",
      "Epoch [13], last_lr: 0.00000, train_loss: 0.0709, val_loss: 0.1594, val_acc: 0.9498\n",
      "Epoch [0], last_lr: 0.00151, train_loss: 0.5725, val_loss: 0.3643, val_acc: 0.8720\n",
      "Epoch [1], last_lr: 0.00435, train_loss: 0.3555, val_loss: 0.4856, val_acc: 0.8296\n",
      "Epoch [2], last_lr: 0.00759, train_loss: 0.3298, val_loss: 0.2534, val_acc: 0.9093\n",
      "Epoch [3], last_lr: 0.00971, train_loss: 0.2911, val_loss: 0.4437, val_acc: 0.8607\n",
      "Epoch [4], last_lr: 0.00994, train_loss: 0.2643, val_loss: 0.2591, val_acc: 0.9098\n",
      "Epoch [5], last_lr: 0.00950, train_loss: 0.2294, val_loss: 0.2638, val_acc: 0.9061\n",
      "Epoch [6], last_lr: 0.00867, train_loss: 0.2059, val_loss: 0.2608, val_acc: 0.9038\n",
      "Epoch [7], last_lr: 0.00750, train_loss: 0.1932, val_loss: 0.2339, val_acc: 0.9196\n",
      "Epoch [8], last_lr: 0.00611, train_loss: 0.1735, val_loss: 0.1795, val_acc: 0.9371\n",
      "Epoch [9], last_lr: 0.00463, train_loss: 0.1586, val_loss: 0.1939, val_acc: 0.9274\n",
      "Epoch [10], last_lr: 0.00317, train_loss: 0.1378, val_loss: 0.1768, val_acc: 0.9384\n",
      "Epoch [11], last_lr: 0.00188, train_loss: 0.1154, val_loss: 0.1610, val_acc: 0.9441\n",
      "Epoch [12], last_lr: 0.00087, train_loss: 0.0937, val_loss: 0.1593, val_acc: 0.9472\n",
      "Epoch [13], last_lr: 0.00022, train_loss: 0.0759, val_loss: 0.1575, val_acc: 0.9500\n",
      "Epoch [14], last_lr: 0.00000, train_loss: 0.0681, val_loss: 0.1575, val_acc: 0.9496\n",
      "Epoch [0], last_lr: 0.00138, train_loss: 0.5962, val_loss: 0.3761, val_acc: 0.8694\n",
      "Epoch [1], last_lr: 0.00394, train_loss: 0.3505, val_loss: 0.7703, val_acc: 0.7892\n",
      "Epoch [2], last_lr: 0.00703, train_loss: 0.3294, val_loss: 0.5967, val_acc: 0.8082\n",
      "Epoch [3], last_lr: 0.00935, train_loss: 0.3218, val_loss: 0.3711, val_acc: 0.8727\n",
      "Epoch [4], last_lr: 0.00999, train_loss: 0.2666, val_loss: 0.3458, val_acc: 0.8862\n",
      "Epoch [5], last_lr: 0.00972, train_loss: 0.2345, val_loss: 0.2480, val_acc: 0.9095\n",
      "Epoch [6], last_lr: 0.00908, train_loss: 0.2153, val_loss: 0.2470, val_acc: 0.9131\n",
      "Epoch [7], last_lr: 0.00812, train_loss: 0.1995, val_loss: 0.2091, val_acc: 0.9265\n",
      "Epoch [8], last_lr: 0.00691, train_loss: 0.1856, val_loss: 0.2099, val_acc: 0.9230\n",
      "Epoch [9], last_lr: 0.00556, train_loss: 0.1651, val_loss: 0.2315, val_acc: 0.9157\n",
      "Epoch [10], last_lr: 0.00416, train_loss: 0.1522, val_loss: 0.1840, val_acc: 0.9350\n",
      "Epoch [11], last_lr: 0.00283, train_loss: 0.1325, val_loss: 0.1799, val_acc: 0.9379\n",
      "Epoch [12], last_lr: 0.00167, train_loss: 0.1100, val_loss: 0.1673, val_acc: 0.9435\n",
      "Epoch [13], last_lr: 0.00077, train_loss: 0.0887, val_loss: 0.1614, val_acc: 0.9459\n",
      "Epoch [14], last_lr: 0.00020, train_loss: 0.0729, val_loss: 0.1576, val_acc: 0.9496\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0660, val_loss: 0.1578, val_acc: 0.9509\n",
      "Epoch [0], last_lr: 0.00127, train_loss: 0.5801, val_loss: 0.3651, val_acc: 0.8674\n",
      "Epoch [1], last_lr: 0.00359, train_loss: 0.3470, val_loss: 0.4966, val_acc: 0.8360\n",
      "Epoch [2], last_lr: 0.00650, train_loss: 0.3222, val_loss: 0.5193, val_acc: 0.8373\n",
      "Epoch [3], last_lr: 0.00894, train_loss: 0.3081, val_loss: 0.3360, val_acc: 0.8800\n",
      "Epoch [4], last_lr: 0.00999, train_loss: 0.2696, val_loss: 0.3226, val_acc: 0.8901\n",
      "Epoch [5], last_lr: 0.00986, train_loss: 0.2448, val_loss: 0.3449, val_acc: 0.8838\n",
      "Epoch [6], last_lr: 0.00938, train_loss: 0.2193, val_loss: 0.2731, val_acc: 0.9070\n",
      "Epoch [7], last_lr: 0.00860, train_loss: 0.2101, val_loss: 0.2597, val_acc: 0.9035\n",
      "Epoch [8], last_lr: 0.00758, train_loss: 0.1929, val_loss: 0.3894, val_acc: 0.8612\n",
      "Epoch [9], last_lr: 0.00637, train_loss: 0.1763, val_loss: 0.2441, val_acc: 0.9170\n",
      "Epoch [10], last_lr: 0.00507, train_loss: 0.1649, val_loss: 0.1933, val_acc: 0.9308\n",
      "Epoch [11], last_lr: 0.00376, train_loss: 0.1421, val_loss: 0.2089, val_acc: 0.9309\n",
      "Epoch [12], last_lr: 0.00254, train_loss: 0.1257, val_loss: 0.1795, val_acc: 0.9384\n",
      "Epoch [13], last_lr: 0.00149, train_loss: 0.1052, val_loss: 0.1657, val_acc: 0.9444\n",
      "Epoch [14], last_lr: 0.00068, train_loss: 0.0845, val_loss: 0.1596, val_acc: 0.9472\n",
      "Epoch [15], last_lr: 0.00017, train_loss: 0.0702, val_loss: 0.1553, val_acc: 0.9499\n",
      "Epoch [16], last_lr: 0.00000, train_loss: 0.0640, val_loss: 0.1536, val_acc: 0.9507\n",
      "Epoch [0], last_lr: 0.00118, train_loss: 0.5536, val_loss: 0.3522, val_acc: 0.8752\n",
      "Epoch [1], last_lr: 0.00329, train_loss: 0.3338, val_loss: 0.3126, val_acc: 0.8894\n",
      "Epoch [2], last_lr: 0.00602, train_loss: 0.3163, val_loss: 0.2978, val_acc: 0.8996\n",
      "Epoch [3], last_lr: 0.00849, train_loss: 0.2978, val_loss: 0.2921, val_acc: 0.8983\n",
      "Epoch [4], last_lr: 0.00987, train_loss: 0.2870, val_loss: 0.2411, val_acc: 0.9165\n",
      "Epoch [5], last_lr: 0.00994, train_loss: 0.2376, val_loss: 0.2810, val_acc: 0.8973\n",
      "Epoch [6], last_lr: 0.00961, train_loss: 0.2202, val_loss: 0.3095, val_acc: 0.8919\n",
      "Epoch [7], last_lr: 0.00899, train_loss: 0.2075, val_loss: 0.2307, val_acc: 0.9211\n",
      "Epoch [8], last_lr: 0.00812, train_loss: 0.1875, val_loss: 0.2413, val_acc: 0.9144\n",
      "Epoch [9], last_lr: 0.00706, train_loss: 0.1831, val_loss: 0.2535, val_acc: 0.9136\n",
      "Epoch [10], last_lr: 0.00587, train_loss: 0.1664, val_loss: 0.2176, val_acc: 0.9239\n",
      "Epoch [11], last_lr: 0.00463, train_loss: 0.1540, val_loss: 0.2070, val_acc: 0.9266\n",
      "Epoch [12], last_lr: 0.00341, train_loss: 0.1356, val_loss: 0.1745, val_acc: 0.9410\n",
      "Epoch [13], last_lr: 0.00229, train_loss: 0.1157, val_loss: 0.1819, val_acc: 0.9393\n",
      "Epoch [14], last_lr: 0.00133, train_loss: 0.0987, val_loss: 0.1700, val_acc: 0.9473\n",
      "Epoch [15], last_lr: 0.00061, train_loss: 0.0780, val_loss: 0.1650, val_acc: 0.9460\n",
      "Epoch [16], last_lr: 0.00015, train_loss: 0.0667, val_loss: 0.1637, val_acc: 0.9496\n",
      "Epoch [17], last_lr: 0.00000, train_loss: 0.0614, val_loss: 0.1620, val_acc: 0.9497\n",
      "Epoch [0], last_lr: 0.00110, train_loss: 0.5578, val_loss: 0.5445, val_acc: 0.8050\n",
      "Epoch [1], last_lr: 0.00302, train_loss: 0.3378, val_loss: 0.3054, val_acc: 0.8866\n",
      "Epoch [2], last_lr: 0.00559, train_loss: 0.3128, val_loss: 0.4047, val_acc: 0.8674\n",
      "Epoch [3], last_lr: 0.00804, train_loss: 0.2895, val_loss: 0.4317, val_acc: 0.8549\n",
      "Epoch [4], last_lr: 0.00965, train_loss: 0.2768, val_loss: 0.2303, val_acc: 0.9197\n",
      "Epoch [5], last_lr: 0.00999, train_loss: 0.2391, val_loss: 0.4168, val_acc: 0.8559\n",
      "Epoch [6], last_lr: 0.00977, train_loss: 0.2212, val_loss: 0.5104, val_acc: 0.8361\n",
      "Epoch [7], last_lr: 0.00928, train_loss: 0.2100, val_loss: 0.2897, val_acc: 0.8908\n",
      "Epoch [8], last_lr: 0.00856, train_loss: 0.1981, val_loss: 0.2311, val_acc: 0.9170\n",
      "Epoch [9], last_lr: 0.00764, train_loss: 0.1843, val_loss: 0.2140, val_acc: 0.9254\n",
      "Epoch [10], last_lr: 0.00657, train_loss: 0.1695, val_loss: 0.2479, val_acc: 0.9141\n",
      "Epoch [11], last_lr: 0.00541, train_loss: 0.1642, val_loss: 0.2177, val_acc: 0.9268\n",
      "Epoch [12], last_lr: 0.00424, train_loss: 0.1476, val_loss: 0.1986, val_acc: 0.9324\n",
      "Epoch [13], last_lr: 0.00310, train_loss: 0.1283, val_loss: 0.1778, val_acc: 0.9401\n",
      "Epoch [14], last_lr: 0.00207, train_loss: 0.1100, val_loss: 0.1683, val_acc: 0.9429\n",
      "Epoch [15], last_lr: 0.00120, train_loss: 0.0931, val_loss: 0.1668, val_acc: 0.9456\n",
      "Epoch [16], last_lr: 0.00055, train_loss: 0.0770, val_loss: 0.1726, val_acc: 0.9456\n",
      "Epoch [17], last_lr: 0.00014, train_loss: 0.0668, val_loss: 0.1619, val_acc: 0.9497\n",
      "Epoch [18], last_lr: 0.00000, train_loss: 0.0604, val_loss: 0.1627, val_acc: 0.9493\n",
      "Epoch [0], last_lr: 0.00103, train_loss: 0.5428, val_loss: 0.3107, val_acc: 0.8863\n",
      "Epoch [1], last_lr: 0.00279, train_loss: 0.3286, val_loss: 0.3361, val_acc: 0.8798\n",
      "Epoch [2], last_lr: 0.00519, train_loss: 0.3009, val_loss: 0.2541, val_acc: 0.9105\n",
      "Epoch [3], last_lr: 0.00759, train_loss: 0.2937, val_loss: 0.3468, val_acc: 0.8821\n",
      "Epoch [4], last_lr: 0.00936, train_loss: 0.2599, val_loss: 0.3862, val_acc: 0.8754\n",
      "Epoch [5], last_lr: 0.01000, train_loss: 0.2561, val_loss: 0.2629, val_acc: 0.9063\n",
      "Epoch [6], last_lr: 0.00987, train_loss: 0.2222, val_loss: 0.2553, val_acc: 0.9132\n",
      "Epoch [7], last_lr: 0.00950, train_loss: 0.2122, val_loss: 0.3044, val_acc: 0.8966\n",
      "Epoch [8], last_lr: 0.00891, train_loss: 0.1979, val_loss: 0.3133, val_acc: 0.9031\n",
      "Epoch [9], last_lr: 0.00812, train_loss: 0.1939, val_loss: 0.3198, val_acc: 0.8900\n",
      "Epoch [10], last_lr: 0.00717, train_loss: 0.1831, val_loss: 0.2270, val_acc: 0.9209\n",
      "Epoch [11], last_lr: 0.00611, train_loss: 0.1707, val_loss: 0.2146, val_acc: 0.9259\n",
      "Epoch [12], last_lr: 0.00500, train_loss: 0.1555, val_loss: 0.2070, val_acc: 0.9280\n",
      "Epoch [13], last_lr: 0.00389, train_loss: 0.1456, val_loss: 0.1852, val_acc: 0.9359\n",
      "Epoch [14], last_lr: 0.00283, train_loss: 0.1253, val_loss: 0.1922, val_acc: 0.9325\n",
      "Epoch [15], last_lr: 0.00188, train_loss: 0.1108, val_loss: 0.1612, val_acc: 0.9444\n",
      "Epoch [16], last_lr: 0.00109, train_loss: 0.0896, val_loss: 0.1684, val_acc: 0.9436\n",
      "Epoch [17], last_lr: 0.00050, train_loss: 0.0752, val_loss: 0.1633, val_acc: 0.9487\n",
      "Epoch [18], last_lr: 0.00013, train_loss: 0.0658, val_loss: 0.1631, val_acc: 0.9498\n",
      "Epoch [19], last_lr: 0.00000, train_loss: 0.0608, val_loss: 0.1616, val_acc: 0.9495\n"
     ]
    }
   ],
   "source": [
    "# testing epochs\n",
    "history_map = {}\n",
    "index = 0\n",
    "history = []\n",
    "for i in range(8,21):\n",
    "    history = []\n",
    "    model = to_device(ResNet9(1, 10), device)\n",
    "    history += fit_one_cycle(i, max_lr, model, train_dl, valid_dl, \n",
    "                                 grad_clip=grad_clip, \n",
    "                                 weight_decay=weight_decay, \n",
    "                                 opt_func=opt_func)\n",
    "    history_map[index] = history\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_transformation(names):\n",
    "    final_val_acc = [history_map[i][-1]['val_acc'] for i in range(len(history_map))]\n",
    "    df = pd.DataFrame({\n",
    "        'Transformation': names,\n",
    "        'Final Validation Accuracy': final_val_acc\n",
    "    })\n",
    "    # Ensure 'Final Validation Accuracy' is treated as numeric\n",
    "    df['Final Validation Accuracy'] = pd.to_numeric(df['Final Validation Accuracy'], errors='coerce')\n",
    "\n",
    "    # Find the row with the highest accuracy\n",
    "    best_row = df.loc[df['Final Validation Accuracy'].idxmax()]\n",
    "    print(f\" best: {best_row['Transformation']} acc: {best_row['Final Validation Accuracy']}\")\n",
    "\n",
    "    # Return the transformation and the best accuracy\n",
    "    return best_row['Transformation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Transformation  Final Validation Loss\n",
      "0                8               0.153036\n",
      "1                9               0.155030\n",
      "2               10               0.152813\n",
      "3               11               0.155522\n",
      "4               12               0.157033\n",
      "5               13               0.155769\n",
      "6               14               0.159400\n",
      "7               15               0.157522\n",
      "8               16               0.157775\n",
      "9               17               0.153617\n",
      "10              18               0.161962\n",
      "11              19               0.162707\n",
      "12              20               0.161572\n",
      "**********\n",
      "    Transformation  Final Validation Accuracy\n",
      "0                8                   0.946204\n",
      "1                9                   0.948118\n",
      "2               10                   0.949113\n",
      "3               11                   0.946815\n",
      "4               12                   0.948391\n",
      "5               13                   0.947317\n",
      "6               14                   0.949771\n",
      "7               15                   0.949576\n",
      "8               16                   0.950923\n",
      "9               17                   0.950676\n",
      "10              18                   0.949725\n",
      "11              19                   0.949276\n",
      "12              20                   0.949452\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_final_val_losses(names):\n",
    "    # Create a list of final validation losses\n",
    "    final_val_losses = [history_map[i][-1]['val_loss'] for i in range(len(history_map))]\n",
    "    final_val_acc = [history_map[i][-1]['val_acc'] for i in range(len(history_map))]\n",
    "\n",
    "    # Create a pandas DataFrame and display it\n",
    "    df = pd.DataFrame({\n",
    "        'Transformation': names,\n",
    "        'Final Validation Loss': final_val_losses\n",
    "    })\n",
    "    print(df)\n",
    "    print(\"*\"*10)\n",
    "    df = pd.DataFrame({\n",
    "        'Transformation': names,\n",
    "        'Final Validation Accuracy': final_val_acc\n",
    "    })\n",
    "    print(df)\n",
    "    \n",
    "compare_final_val_losses(range(8,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00014, train_loss: 0.7385, val_loss: 0.3612, val_acc: 0.8641\n",
      "Epoch [1], last_lr: 0.00039, train_loss: 0.3774, val_loss: 0.2937, val_acc: 0.8972\n",
      "Epoch [2], last_lr: 0.00070, train_loss: 0.3140, val_loss: 0.3416, val_acc: 0.8793\n",
      "Epoch [3], last_lr: 0.00094, train_loss: 0.2775, val_loss: 0.2681, val_acc: 0.9056\n",
      "Epoch [4], last_lr: 0.00100, train_loss: 0.2411, val_loss: 0.2715, val_acc: 0.9039\n",
      "Epoch [5], last_lr: 0.00097, train_loss: 0.2111, val_loss: 0.2457, val_acc: 0.9105\n",
      "Epoch [6], last_lr: 0.00091, train_loss: 0.1879, val_loss: 0.1863, val_acc: 0.9305\n",
      "Epoch [7], last_lr: 0.00081, train_loss: 0.1653, val_loss: 0.2193, val_acc: 0.9233\n",
      "Epoch [8], last_lr: 0.00069, train_loss: 0.1513, val_loss: 0.2204, val_acc: 0.9230\n",
      "Epoch [9], last_lr: 0.00056, train_loss: 0.1320, val_loss: 0.2485, val_acc: 0.9195\n",
      "Epoch [10], last_lr: 0.00042, train_loss: 0.1182, val_loss: 0.1818, val_acc: 0.9427\n",
      "Epoch [11], last_lr: 0.00028, train_loss: 0.1030, val_loss: 0.1755, val_acc: 0.9434\n",
      "Epoch [12], last_lr: 0.00017, train_loss: 0.0908, val_loss: 0.1772, val_acc: 0.9443\n",
      "Epoch [13], last_lr: 0.00008, train_loss: 0.0803, val_loss: 0.1788, val_acc: 0.9459\n",
      "Epoch [14], last_lr: 0.00002, train_loss: 0.0727, val_loss: 0.1731, val_acc: 0.9481\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0711, val_loss: 0.1734, val_acc: 0.9488\n",
      "Epoch [0], last_lr: 0.00028, train_loss: 0.6368, val_loss: 0.3343, val_acc: 0.8803\n",
      "Epoch [1], last_lr: 0.00079, train_loss: 0.3605, val_loss: 0.3074, val_acc: 0.8875\n",
      "Epoch [2], last_lr: 0.00141, train_loss: 0.2990, val_loss: 0.2788, val_acc: 0.8957\n",
      "Epoch [3], last_lr: 0.00187, train_loss: 0.2674, val_loss: 0.2564, val_acc: 0.9094\n",
      "Epoch [4], last_lr: 0.00200, train_loss: 0.2326, val_loss: 0.2295, val_acc: 0.9192\n",
      "Epoch [5], last_lr: 0.00194, train_loss: 0.2044, val_loss: 0.2589, val_acc: 0.9109\n",
      "Epoch [6], last_lr: 0.00182, train_loss: 0.1846, val_loss: 0.2251, val_acc: 0.9222\n",
      "Epoch [7], last_lr: 0.00162, train_loss: 0.1625, val_loss: 0.2800, val_acc: 0.8956\n",
      "Epoch [8], last_lr: 0.00138, train_loss: 0.1495, val_loss: 0.2217, val_acc: 0.9264\n",
      "Epoch [9], last_lr: 0.00111, train_loss: 0.1334, val_loss: 0.1901, val_acc: 0.9361\n",
      "Epoch [10], last_lr: 0.00083, train_loss: 0.1164, val_loss: 0.1825, val_acc: 0.9381\n",
      "Epoch [11], last_lr: 0.00057, train_loss: 0.0983, val_loss: 0.1682, val_acc: 0.9423\n",
      "Epoch [12], last_lr: 0.00033, train_loss: 0.0864, val_loss: 0.1815, val_acc: 0.9454\n",
      "Epoch [13], last_lr: 0.00015, train_loss: 0.0743, val_loss: 0.1713, val_acc: 0.9482\n",
      "Epoch [14], last_lr: 0.00004, train_loss: 0.0683, val_loss: 0.1709, val_acc: 0.9491\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0649, val_loss: 0.1715, val_acc: 0.9509\n",
      "Epoch [0], last_lr: 0.00041, train_loss: 0.5548, val_loss: 0.4706, val_acc: 0.8316\n",
      "Epoch [1], last_lr: 0.00118, train_loss: 0.3419, val_loss: 0.4168, val_acc: 0.8519\n",
      "Epoch [2], last_lr: 0.00211, train_loss: 0.2962, val_loss: 0.2958, val_acc: 0.8961\n",
      "Epoch [3], last_lr: 0.00281, train_loss: 0.2649, val_loss: 0.4771, val_acc: 0.8355\n",
      "Epoch [4], last_lr: 0.00300, train_loss: 0.2326, val_loss: 0.2223, val_acc: 0.9224\n",
      "Epoch [5], last_lr: 0.00292, train_loss: 0.2069, val_loss: 0.3376, val_acc: 0.8894\n",
      "Epoch [6], last_lr: 0.00272, train_loss: 0.1811, val_loss: 0.2409, val_acc: 0.9183\n",
      "Epoch [7], last_lr: 0.00244, train_loss: 0.1705, val_loss: 0.2349, val_acc: 0.9212\n",
      "Epoch [8], last_lr: 0.00207, train_loss: 0.1509, val_loss: 0.2221, val_acc: 0.9241\n",
      "Epoch [9], last_lr: 0.00167, train_loss: 0.1293, val_loss: 0.1865, val_acc: 0.9393\n",
      "Epoch [10], last_lr: 0.00125, train_loss: 0.1169, val_loss: 0.2004, val_acc: 0.9332\n",
      "Epoch [11], last_lr: 0.00085, train_loss: 0.0998, val_loss: 0.1764, val_acc: 0.9441\n",
      "Epoch [12], last_lr: 0.00050, train_loss: 0.0856, val_loss: 0.1817, val_acc: 0.9447\n",
      "Epoch [13], last_lr: 0.00023, train_loss: 0.0747, val_loss: 0.1753, val_acc: 0.9471\n",
      "Epoch [14], last_lr: 0.00006, train_loss: 0.0654, val_loss: 0.1697, val_acc: 0.9491\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0613, val_loss: 0.1725, val_acc: 0.9488\n",
      "Epoch [0], last_lr: 0.00055, train_loss: 0.5485, val_loss: 0.4240, val_acc: 0.8404\n",
      "Epoch [1], last_lr: 0.00158, train_loss: 0.3397, val_loss: 0.3273, val_acc: 0.8829\n",
      "Epoch [2], last_lr: 0.00281, train_loss: 0.3003, val_loss: 0.3116, val_acc: 0.8942\n",
      "Epoch [3], last_lr: 0.00374, train_loss: 0.2732, val_loss: 0.5350, val_acc: 0.8278\n",
      "Epoch [4], last_lr: 0.00400, train_loss: 0.2355, val_loss: 0.3320, val_acc: 0.8830\n",
      "Epoch [5], last_lr: 0.00389, train_loss: 0.2142, val_loss: 0.2089, val_acc: 0.9267\n",
      "Epoch [6], last_lr: 0.00363, train_loss: 0.1863, val_loss: 0.2426, val_acc: 0.9127\n",
      "Epoch [7], last_lr: 0.00325, train_loss: 0.1702, val_loss: 0.1892, val_acc: 0.9330\n",
      "Epoch [8], last_lr: 0.00277, train_loss: 0.1542, val_loss: 0.2150, val_acc: 0.9234\n",
      "Epoch [9], last_lr: 0.00222, train_loss: 0.1383, val_loss: 0.2009, val_acc: 0.9333\n",
      "Epoch [10], last_lr: 0.00166, train_loss: 0.1198, val_loss: 0.1795, val_acc: 0.9408\n",
      "Epoch [11], last_lr: 0.00113, train_loss: 0.1034, val_loss: 0.1765, val_acc: 0.9442\n",
      "Epoch [12], last_lr: 0.00067, train_loss: 0.0882, val_loss: 0.1747, val_acc: 0.9440\n",
      "Epoch [13], last_lr: 0.00031, train_loss: 0.0744, val_loss: 0.1708, val_acc: 0.9470\n",
      "Epoch [14], last_lr: 0.00008, train_loss: 0.0655, val_loss: 0.1649, val_acc: 0.9496\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0603, val_loss: 0.1657, val_acc: 0.9505\n",
      "Epoch [0], last_lr: 0.00069, train_loss: 0.5295, val_loss: 0.3520, val_acc: 0.8720\n",
      "Epoch [1], last_lr: 0.00197, train_loss: 0.3444, val_loss: 0.3705, val_acc: 0.8699\n",
      "Epoch [2], last_lr: 0.00351, train_loss: 0.2994, val_loss: 0.2485, val_acc: 0.9109\n",
      "Epoch [3], last_lr: 0.00468, train_loss: 0.2748, val_loss: 0.2703, val_acc: 0.9080\n",
      "Epoch [4], last_lr: 0.00500, train_loss: 0.2504, val_loss: 0.3214, val_acc: 0.8921\n",
      "Epoch [5], last_lr: 0.00486, train_loss: 0.2120, val_loss: 0.2860, val_acc: 0.9055\n",
      "Epoch [6], last_lr: 0.00454, train_loss: 0.1980, val_loss: 0.4347, val_acc: 0.8721\n",
      "Epoch [7], last_lr: 0.00406, train_loss: 0.1794, val_loss: 0.2040, val_acc: 0.9283\n",
      "Epoch [8], last_lr: 0.00346, train_loss: 0.1620, val_loss: 0.2700, val_acc: 0.9072\n",
      "Epoch [9], last_lr: 0.00278, train_loss: 0.1448, val_loss: 0.1799, val_acc: 0.9372\n",
      "Epoch [10], last_lr: 0.00208, train_loss: 0.1257, val_loss: 0.2282, val_acc: 0.9251\n",
      "Epoch [11], last_lr: 0.00142, train_loss: 0.1081, val_loss: 0.1757, val_acc: 0.9423\n",
      "Epoch [12], last_lr: 0.00083, train_loss: 0.0905, val_loss: 0.1687, val_acc: 0.9459\n",
      "Epoch [13], last_lr: 0.00038, train_loss: 0.0760, val_loss: 0.1709, val_acc: 0.9498\n",
      "Epoch [14], last_lr: 0.00010, train_loss: 0.0669, val_loss: 0.1671, val_acc: 0.9499\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0603, val_loss: 0.1675, val_acc: 0.9501\n",
      "Epoch [0], last_lr: 0.00083, train_loss: 0.5457, val_loss: 0.4278, val_acc: 0.8507\n",
      "Epoch [1], last_lr: 0.00237, train_loss: 0.3335, val_loss: 0.3967, val_acc: 0.8631\n",
      "Epoch [2], last_lr: 0.00422, train_loss: 0.3012, val_loss: 0.3113, val_acc: 0.8925\n",
      "Epoch [3], last_lr: 0.00561, train_loss: 0.2803, val_loss: 0.7741, val_acc: 0.7840\n",
      "Epoch [4], last_lr: 0.00600, train_loss: 0.2526, val_loss: 0.2880, val_acc: 0.9026\n",
      "Epoch [5], last_lr: 0.00583, train_loss: 0.2169, val_loss: 0.4339, val_acc: 0.8586\n",
      "Epoch [6], last_lr: 0.00545, train_loss: 0.1991, val_loss: 0.2007, val_acc: 0.9256\n",
      "Epoch [7], last_lr: 0.00487, train_loss: 0.1792, val_loss: 0.2398, val_acc: 0.9155\n",
      "Epoch [8], last_lr: 0.00415, train_loss: 0.1610, val_loss: 0.2023, val_acc: 0.9301\n",
      "Epoch [9], last_lr: 0.00334, train_loss: 0.1456, val_loss: 0.2050, val_acc: 0.9309\n",
      "Epoch [10], last_lr: 0.00250, train_loss: 0.1334, val_loss: 0.1881, val_acc: 0.9379\n",
      "Epoch [11], last_lr: 0.00170, train_loss: 0.1098, val_loss: 0.1760, val_acc: 0.9407\n",
      "Epoch [12], last_lr: 0.00100, train_loss: 0.0939, val_loss: 0.1757, val_acc: 0.9438\n",
      "Epoch [13], last_lr: 0.00046, train_loss: 0.0775, val_loss: 0.1741, val_acc: 0.9445\n",
      "Epoch [14], last_lr: 0.00012, train_loss: 0.0660, val_loss: 0.1696, val_acc: 0.9487\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0605, val_loss: 0.1710, val_acc: 0.9482\n",
      "Epoch [0], last_lr: 0.00097, train_loss: 0.5357, val_loss: 0.3163, val_acc: 0.8869\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.3437, val_loss: 0.3769, val_acc: 0.8731\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.3028, val_loss: 0.2850, val_acc: 0.9032\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.2819, val_loss: 0.3917, val_acc: 0.8680\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2666, val_loss: 0.2699, val_acc: 0.9035\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2242, val_loss: 0.2675, val_acc: 0.9063\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.1986, val_loss: 0.4425, val_acc: 0.8473\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.1862, val_loss: 0.2415, val_acc: 0.9210\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.1713, val_loss: 0.2411, val_acc: 0.9136\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.1530, val_loss: 0.1941, val_acc: 0.9335\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1342, val_loss: 0.1798, val_acc: 0.9418\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.1150, val_loss: 0.1740, val_acc: 0.9413\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.0983, val_loss: 0.1633, val_acc: 0.9439\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.0788, val_loss: 0.1767, val_acc: 0.9449\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.0671, val_loss: 0.1680, val_acc: 0.9513\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0612, val_loss: 0.1659, val_acc: 0.9527\n",
      "Epoch [0], last_lr: 0.00110, train_loss: 0.5363, val_loss: 0.3310, val_acc: 0.8783\n",
      "Epoch [1], last_lr: 0.00315, train_loss: 0.3460, val_loss: 0.3447, val_acc: 0.8801\n",
      "Epoch [2], last_lr: 0.00562, train_loss: 0.3115, val_loss: 0.4402, val_acc: 0.8580\n",
      "Epoch [3], last_lr: 0.00748, train_loss: 0.2838, val_loss: 0.3019, val_acc: 0.8895\n",
      "Epoch [4], last_lr: 0.00799, train_loss: 0.2524, val_loss: 0.4509, val_acc: 0.8477\n",
      "Epoch [5], last_lr: 0.00778, train_loss: 0.2282, val_loss: 0.2443, val_acc: 0.9124\n",
      "Epoch [6], last_lr: 0.00726, train_loss: 0.2111, val_loss: 0.3192, val_acc: 0.8903\n",
      "Epoch [7], last_lr: 0.00649, train_loss: 0.1899, val_loss: 0.2240, val_acc: 0.9172\n",
      "Epoch [8], last_lr: 0.00553, train_loss: 0.1707, val_loss: 0.3514, val_acc: 0.8953\n",
      "Epoch [9], last_lr: 0.00445, train_loss: 0.1622, val_loss: 0.1835, val_acc: 0.9352\n",
      "Epoch [10], last_lr: 0.00333, train_loss: 0.1403, val_loss: 0.2064, val_acc: 0.9294\n",
      "Epoch [11], last_lr: 0.00226, train_loss: 0.1253, val_loss: 0.1758, val_acc: 0.9396\n",
      "Epoch [12], last_lr: 0.00133, train_loss: 0.1010, val_loss: 0.1720, val_acc: 0.9443\n",
      "Epoch [13], last_lr: 0.00061, train_loss: 0.0834, val_loss: 0.1711, val_acc: 0.9459\n",
      "Epoch [14], last_lr: 0.00016, train_loss: 0.0695, val_loss: 0.1684, val_acc: 0.9489\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0631, val_loss: 0.1661, val_acc: 0.9502\n",
      "Epoch [0], last_lr: 0.00124, train_loss: 0.5511, val_loss: 0.4464, val_acc: 0.8458\n",
      "Epoch [1], last_lr: 0.00355, train_loss: 0.3398, val_loss: 0.9572, val_acc: 0.7309\n",
      "Epoch [2], last_lr: 0.00632, train_loss: 0.3159, val_loss: 0.3737, val_acc: 0.8746\n",
      "Epoch [3], last_lr: 0.00842, train_loss: 0.2880, val_loss: 0.4828, val_acc: 0.8415\n",
      "Epoch [4], last_lr: 0.00899, train_loss: 0.2732, val_loss: 0.2616, val_acc: 0.9047\n",
      "Epoch [5], last_lr: 0.00875, train_loss: 0.2219, val_loss: 0.2377, val_acc: 0.9142\n",
      "Epoch [6], last_lr: 0.00817, train_loss: 0.2108, val_loss: 0.2768, val_acc: 0.9028\n",
      "Epoch [7], last_lr: 0.00731, train_loss: 0.2012, val_loss: 0.2448, val_acc: 0.9093\n",
      "Epoch [8], last_lr: 0.00622, train_loss: 0.1762, val_loss: 0.2329, val_acc: 0.9175\n",
      "Epoch [9], last_lr: 0.00500, train_loss: 0.1649, val_loss: 0.2782, val_acc: 0.9079\n",
      "Epoch [10], last_lr: 0.00375, train_loss: 0.1477, val_loss: 0.1964, val_acc: 0.9340\n",
      "Epoch [11], last_lr: 0.00255, train_loss: 0.1305, val_loss: 0.1626, val_acc: 0.9428\n",
      "Epoch [12], last_lr: 0.00150, train_loss: 0.1061, val_loss: 0.1713, val_acc: 0.9426\n",
      "Epoch [13], last_lr: 0.00069, train_loss: 0.0860, val_loss: 0.1556, val_acc: 0.9494\n",
      "Epoch [14], last_lr: 0.00018, train_loss: 0.0718, val_loss: 0.1559, val_acc: 0.9507\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0638, val_loss: 0.1548, val_acc: 0.9520\n",
      "Epoch [0], last_lr: 0.00138, train_loss: 0.5796, val_loss: 0.3669, val_acc: 0.8676\n",
      "Epoch [1], last_lr: 0.00394, train_loss: 0.3370, val_loss: 0.4020, val_acc: 0.8729\n",
      "Epoch [2], last_lr: 0.00703, train_loss: 0.3150, val_loss: 0.7597, val_acc: 0.7891\n",
      "Epoch [3], last_lr: 0.00935, train_loss: 0.2932, val_loss: 0.3139, val_acc: 0.8965\n",
      "Epoch [4], last_lr: 0.00999, train_loss: 0.2626, val_loss: 0.4338, val_acc: 0.8490\n",
      "Epoch [5], last_lr: 0.00972, train_loss: 0.2329, val_loss: 0.2977, val_acc: 0.8940\n",
      "Epoch [6], last_lr: 0.00908, train_loss: 0.2083, val_loss: 0.2929, val_acc: 0.8919\n",
      "Epoch [7], last_lr: 0.00812, train_loss: 0.2029, val_loss: 0.3632, val_acc: 0.8766\n",
      "Epoch [8], last_lr: 0.00691, train_loss: 0.1858, val_loss: 0.1862, val_acc: 0.9334\n",
      "Epoch [9], last_lr: 0.00556, train_loss: 0.1652, val_loss: 0.1866, val_acc: 0.9318\n",
      "Epoch [10], last_lr: 0.00416, train_loss: 0.1487, val_loss: 0.2027, val_acc: 0.9304\n",
      "Epoch [11], last_lr: 0.00283, train_loss: 0.1309, val_loss: 0.1951, val_acc: 0.9379\n",
      "Epoch [12], last_lr: 0.00167, train_loss: 0.1082, val_loss: 0.1691, val_acc: 0.9424\n",
      "Epoch [13], last_lr: 0.00077, train_loss: 0.0893, val_loss: 0.1624, val_acc: 0.9471\n",
      "Epoch [14], last_lr: 0.00020, train_loss: 0.0719, val_loss: 0.1596, val_acc: 0.9497\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0649, val_loss: 0.1602, val_acc: 0.9500\n",
      "   Transformation  Final Validation Loss\n",
      "0           0.001               0.173405\n",
      "1           0.002               0.171454\n",
      "2           0.003               0.172528\n",
      "3           0.004               0.165747\n",
      "4           0.005               0.167490\n",
      "5           0.006               0.171002\n",
      "6           0.007               0.165905\n",
      "7           0.008               0.166061\n",
      "8           0.009               0.154839\n",
      "9           0.010               0.160228\n",
      "**********\n",
      "   Transformation  Final Validation Accuracy\n",
      "0           0.001                   0.948807\n",
      "1           0.002                   0.950930\n",
      "2           0.003                   0.948756\n",
      "3           0.004                   0.950467\n",
      "4           0.005                   0.950077\n",
      "5           0.006                   0.948157\n",
      "6           0.007                   0.952707\n",
      "7           0.008                   0.950194\n",
      "8           0.009                   0.952049\n",
      "9           0.010                   0.950005\n"
     ]
    }
   ],
   "source": [
    "# best lr\n",
    "\n",
    "import numpy as np\n",
    "best_epoch = 16\n",
    "history_map = {}\n",
    "index = 0\n",
    "\n",
    "for i in np.arange(0.001, 0.011, 0.001):\n",
    "    history = []\n",
    "    model = to_device(ResNet9(1, 10), device)\n",
    "    history += fit_one_cycle(best_epoch, i, model, train_dl, valid_dl, \n",
    "                                 grad_clip=grad_clip, \n",
    "                                 weight_decay=weight_decay, \n",
    "                                 opt_func=opt_func)\n",
    "    history_map[index] = history\n",
    "    index += 1\n",
    "\n",
    "compare_final_val_losses(np.arange(0.001, 0.011, 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best: 0.007 acc: 0.9527066946029663\n",
      "Epoch [0], last_lr: 0.00097, train_loss: 0.5355, val_loss: 0.3262, val_acc: 0.8862\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.3327, val_loss: 0.6080, val_acc: 0.7970\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.3061, val_loss: 0.3920, val_acc: 0.8719\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.2791, val_loss: 0.3198, val_acc: 0.8900\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2534, val_loss: 0.3363, val_acc: 0.8797\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2214, val_loss: 0.2386, val_acc: 0.9162\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.1994, val_loss: 0.2960, val_acc: 0.8974\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.1920, val_loss: 0.2008, val_acc: 0.9254\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.1670, val_loss: 0.1820, val_acc: 0.9370\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.1506, val_loss: 0.2231, val_acc: 0.9243\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1319, val_loss: 0.1920, val_acc: 0.9347\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.1188, val_loss: 0.1862, val_acc: 0.9391\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.0984, val_loss: 0.1678, val_acc: 0.9443\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.0800, val_loss: 0.1680, val_acc: 0.9480\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.0678, val_loss: 0.1645, val_acc: 0.9495\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0608, val_loss: 0.1650, val_acc: 0.9507\n",
      "Epoch [0], last_lr: 0.00097, train_loss: 0.5593, val_loss: 0.4065, val_acc: 0.8601\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.3448, val_loss: 0.2867, val_acc: 0.8964\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.3037, val_loss: 0.3519, val_acc: 0.8767\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.2929, val_loss: 0.5514, val_acc: 0.8449\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2600, val_loss: 0.2836, val_acc: 0.8993\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2332, val_loss: 0.3438, val_acc: 0.8749\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.2150, val_loss: 0.3083, val_acc: 0.8904\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.2032, val_loss: 0.2814, val_acc: 0.8996\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.1883, val_loss: 0.2704, val_acc: 0.9043\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.1692, val_loss: 0.1956, val_acc: 0.9284\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1518, val_loss: 0.1846, val_acc: 0.9335\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.1341, val_loss: 0.1813, val_acc: 0.9380\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.1125, val_loss: 0.1692, val_acc: 0.9415\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.0905, val_loss: 0.1595, val_acc: 0.9457\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.0741, val_loss: 0.1593, val_acc: 0.9479\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0667, val_loss: 0.1594, val_acc: 0.9482\n",
      "Epoch [0], last_lr: 0.00097, train_loss: 0.6009, val_loss: 0.4216, val_acc: 0.8514\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.3483, val_loss: 0.4571, val_acc: 0.8431\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.3106, val_loss: 0.4243, val_acc: 0.8529\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.2917, val_loss: 0.3256, val_acc: 0.8821\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2700, val_loss: 0.4114, val_acc: 0.8580\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2396, val_loss: 0.4078, val_acc: 0.8642\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.2238, val_loss: 0.3533, val_acc: 0.8816\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.2122, val_loss: 0.2163, val_acc: 0.9206\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.2014, val_loss: 0.2224, val_acc: 0.9237\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.1847, val_loss: 0.1964, val_acc: 0.9298\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1671, val_loss: 0.1858, val_acc: 0.9333\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.1478, val_loss: 0.1769, val_acc: 0.9351\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.1230, val_loss: 0.1582, val_acc: 0.9420\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.0997, val_loss: 0.1600, val_acc: 0.9468\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.0805, val_loss: 0.1567, val_acc: 0.9498\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0708, val_loss: 0.1567, val_acc: 0.9505\n",
      "Epoch [0], last_lr: 0.00097, train_loss: 0.5575, val_loss: 0.4196, val_acc: 0.8597\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.3530, val_loss: 0.4142, val_acc: 0.8609\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.3059, val_loss: 0.2551, val_acc: 0.9043\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.2978, val_loss: 0.3499, val_acc: 0.8718\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2734, val_loss: 0.3402, val_acc: 0.8835\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2486, val_loss: 0.3243, val_acc: 0.8847\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.2351, val_loss: 0.2610, val_acc: 0.9072\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.2205, val_loss: 0.3230, val_acc: 0.8888\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.2054, val_loss: 0.2981, val_acc: 0.8954\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.1971, val_loss: 0.2130, val_acc: 0.9230\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1788, val_loss: 0.2471, val_acc: 0.9116\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.1602, val_loss: 0.1903, val_acc: 0.9316\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.1369, val_loss: 0.1797, val_acc: 0.9375\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.1111, val_loss: 0.1620, val_acc: 0.9467\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.0892, val_loss: 0.1532, val_acc: 0.9478\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0781, val_loss: 0.1541, val_acc: 0.9503\n",
      "Epoch [0], last_lr: 0.00097, train_loss: 0.5588, val_loss: 0.4603, val_acc: 0.8376\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.3446, val_loss: 0.3545, val_acc: 0.8790\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.3102, val_loss: 0.3530, val_acc: 0.8640\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.3022, val_loss: 0.6425, val_acc: 0.8166\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2833, val_loss: 0.5186, val_acc: 0.8207\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2538, val_loss: 0.4617, val_acc: 0.8404\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.2426, val_loss: 0.3582, val_acc: 0.8757\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.2281, val_loss: 0.2456, val_acc: 0.9101\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.2216, val_loss: 0.2379, val_acc: 0.9159\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.2036, val_loss: 0.2303, val_acc: 0.9185\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1900, val_loss: 0.2807, val_acc: 0.8975\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.1708, val_loss: 0.1923, val_acc: 0.9325\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.1451, val_loss: 0.1753, val_acc: 0.9381\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.1201, val_loss: 0.1682, val_acc: 0.9407\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.0943, val_loss: 0.1527, val_acc: 0.9488\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0811, val_loss: 0.1515, val_acc: 0.9488\n"
     ]
    }
   ],
   "source": [
    "# best weight decay\n",
    "\n",
    "import numpy as np\n",
    "best_lr = find_best_transformation(np.arange(0.001, 0.011, 0.001))\n",
    "history_map = {}\n",
    "index = 0\n",
    "\n",
    "for i in np.arange(1e-4, 6e-4, 1e-4):\n",
    "    history = []\n",
    "    model = to_device(ResNet9(1, 10), device)\n",
    "    history += fit_one_cycle(best_epoch, best_lr, model, train_dl, valid_dl, \n",
    "                                 grad_clip=grad_clip, \n",
    "                                 weight_decay=i, \n",
    "                                 opt_func=opt_func)\n",
    "    history_map[index] = history\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_final_val_losses(np.arange(1e-4, 6e-4, 1e-4))\n",
    "best_weight_decay = find_best_transformation(np.arange(1e-4, 6e-4, 1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00097, train_loss: 0.5353, val_loss: 0.4381, val_acc: 0.8431\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.3424, val_loss: 0.3907, val_acc: 0.8555\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.3115, val_loss: 0.8307, val_acc: 0.7607\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.2813, val_loss: 0.3088, val_acc: 0.8870\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2517, val_loss: 0.2469, val_acc: 0.9115\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2232, val_loss: 0.2203, val_acc: 0.9198\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.2054, val_loss: 0.2334, val_acc: 0.9167\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.1836, val_loss: 0.2436, val_acc: 0.9125\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.1687, val_loss: 0.2235, val_acc: 0.9248\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.1575, val_loss: 0.2424, val_acc: 0.9166\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1347, val_loss: 0.1841, val_acc: 0.9393\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.1160, val_loss: 0.2005, val_acc: 0.9363\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.0997, val_loss: 0.1708, val_acc: 0.9437\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.0797, val_loss: 0.1701, val_acc: 0.9449\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.0679, val_loss: 0.1700, val_acc: 0.9493\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0614, val_loss: 0.1669, val_acc: 0.9504\n",
      "Epoch [0], last_lr: 0.00097, train_loss: 0.5417, val_loss: 0.6524, val_acc: 0.7737\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.3466, val_loss: 0.3096, val_acc: 0.8890\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.2949, val_loss: 0.3551, val_acc: 0.8824\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.2934, val_loss: 0.3168, val_acc: 0.8915\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2393, val_loss: 0.3533, val_acc: 0.8915\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2104, val_loss: 0.2520, val_acc: 0.9109\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.1732, val_loss: 0.1936, val_acc: 0.9315\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.1586, val_loss: 0.1957, val_acc: 0.9312\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.1369, val_loss: 0.1991, val_acc: 0.9376\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.1194, val_loss: 0.1795, val_acc: 0.9424\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1024, val_loss: 0.1749, val_acc: 0.9453\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.0900, val_loss: 0.1798, val_acc: 0.9447\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.0775, val_loss: 0.1740, val_acc: 0.9496\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.0676, val_loss: 0.1732, val_acc: 0.9486\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.0616, val_loss: 0.1755, val_acc: 0.9507\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.0582, val_loss: 0.1757, val_acc: 0.9509\n",
      "Epoch [0], last_lr: 0.00097, train_loss: 1.0188, val_loss: 0.4553, val_acc: 0.8404\n",
      "Epoch [1], last_lr: 0.00276, train_loss: 0.4535, val_loss: 0.4336, val_acc: 0.8374\n",
      "Epoch [2], last_lr: 0.00492, train_loss: 0.3675, val_loss: 0.4870, val_acc: 0.8493\n",
      "Epoch [3], last_lr: 0.00655, train_loss: 0.3257, val_loss: 0.2937, val_acc: 0.8900\n",
      "Epoch [4], last_lr: 0.00699, train_loss: 0.2826, val_loss: 0.3175, val_acc: 0.8841\n",
      "Epoch [5], last_lr: 0.00680, train_loss: 0.2587, val_loss: 0.3209, val_acc: 0.8868\n",
      "Epoch [6], last_lr: 0.00635, train_loss: 0.2326, val_loss: 0.2976, val_acc: 0.9003\n",
      "Epoch [7], last_lr: 0.00568, train_loss: 0.2177, val_loss: 0.2230, val_acc: 0.9196\n",
      "Epoch [8], last_lr: 0.00484, train_loss: 0.2002, val_loss: 0.2105, val_acc: 0.9267\n",
      "Epoch [9], last_lr: 0.00389, train_loss: 0.1825, val_loss: 0.2177, val_acc: 0.9258\n",
      "Epoch [10], last_lr: 0.00291, train_loss: 0.1681, val_loss: 0.2303, val_acc: 0.9230\n",
      "Epoch [11], last_lr: 0.00198, train_loss: 0.1536, val_loss: 0.1952, val_acc: 0.9320\n",
      "Epoch [12], last_lr: 0.00117, train_loss: 0.1446, val_loss: 0.1965, val_acc: 0.9317\n",
      "Epoch [13], last_lr: 0.00054, train_loss: 0.1319, val_loss: 0.1965, val_acc: 0.9322\n",
      "Epoch [14], last_lr: 0.00014, train_loss: 0.1246, val_loss: 0.1916, val_acc: 0.9351\n",
      "Epoch [15], last_lr: 0.00000, train_loss: 0.1209, val_loss: 0.1907, val_acc: 0.9343\n",
      "  Transformation  Final Validation Loss\n",
      "0           Adam               0.166921\n",
      "1          AdamW               0.175656\n",
      "2            SGD               0.190734\n",
      "**********\n",
      "  Transformation  Final Validation Accuracy\n",
      "0           Adam                   0.950370\n",
      "1          AdamW                   0.950949\n",
      "2            SGD                   0.934323\n"
     ]
    }
   ],
   "source": [
    "# best optimizer\n",
    "history_map = {}\n",
    "best_weight_decay = 1e-4\n",
    "\n",
    "opt_func_list = [\"Adam\", \"AdamW\", \"SGD\"]\n",
    "\n",
    "history = []\n",
    "model = to_device(ResNet9(1, 10), device)\n",
    "history += fit_one_cycle(best_epoch, best_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=best_weight_decay, \n",
    "                             opt_func=torch.optim.Adam)\n",
    "history_map[0] = history\n",
    "\n",
    "history = []\n",
    "model = to_device(ResNet9(1, 10), device)\n",
    "history += fit_one_cycle(best_epoch, best_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=best_weight_decay, \n",
    "                             opt_func=torch.optim.AdamW)\n",
    "history_map[1] = history\n",
    "\n",
    "history = []\n",
    "model = to_device(ResNet9(1, 10), device)\n",
    "history += fit_one_cycle(best_epoch, best_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=best_weight_decay, \n",
    "                             opt_func=torch.optim.SGD)\n",
    "history_map[2] = history\n",
    "\n",
    "compare_final_val_losses(opt_func_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
